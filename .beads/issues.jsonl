{"id":"vc-01e","title":"Epic: Testing \u0026 Deployment","description":"Set up testing infrastructure and deployment configuration for the Voice Gateway and web client.\n\n## Background\nThe V0 has no tests. The new architecture should have unit tests for critical components (turn state machine, phrase chunker, rolling-window STT) and integration tests for the WebSocket protocol.","status":"open","priority":2,"issue_type":"epic","owner":"trevor.j.atkinson@gmail.com","created_at":"2026-02-05T17:26:08.864923-08:00","created_by":"Trevor","updated_at":"2026-02-05T17:26:08.864923-08:00","labels":["deployment","epic","testing"]}
{"id":"vc-01e.1","title":"Unit tests: turn state machine, phrase chunker, stable prefix algorithm","description":"## What\nWrite unit tests for the three most critical and algorithmically complex components.\n\n## Test Suites\n\n### Turn State Machine Tests\n- All valid transitions succeed\n- Invalid transitions rejected (e.g., IDLE → THINKING)\n- Auto-send timer fires after configured delay\n- Auto-send timer cleared on manual send\n- Barge-in during SPEAKING transitions correctly\n- Reset clears all state\n\n### Phrase Chunker Tests\n- Simple sentence splitting: 'Hello! How are you?' → ['Hello!', 'How are you?']\n- Abbreviation handling: 'Dr. Smith said hello.' → ['Dr. Smith said hello.'] (not split after Dr.)\n- Long sentence fallback: 100+ char sentence with comma → splits at comma\n- Minimum chunk size: 'Hi. Ok. Yes.' → merged appropriately\n- Maximum chunk size: 200+ char text → split\n- Code block preservation: text with ``` blocks → code not split\n- URL preservation: text with https://... → URL not split\n- Final flush: remaining buffer emitted on isFinal=true\n- Empty input handling\n- Emoji handling\n\n### Stable Prefix Algorithm Tests  \n- First decode: no stable prefix\n- Two matching decodes: common prefix becomes stable\n- Prefix snaps to word boundary\n- Prefix grows monotonically (never shrinks)\n- Complete transcript: stable = full text, unstable = ''\n- Divergent decodes: stable stays at last confirmed point\n\n## Framework\nUse vitest (fast, TypeScript-native, good for unit tests).\n\n## Acceptance Criteria\n- [ ] \u003e90% branch coverage for turn machine\n- [ ] \u003e90% branch coverage for phrase chunker\n- [ ] \u003e90% branch coverage for stable prefix algorithm\n- [ ] Tests run in \u003c5s\n- [ ] CI-compatible (no external dependencies needed)","status":"closed","priority":2,"issue_type":"task","owner":"trevor.j.atkinson@gmail.com","created_at":"2026-02-05T17:26:09.03341-08:00","created_by":"Trevor","updated_at":"2026-02-05T23:12:44.690182-08:00","closed_at":"2026-02-05T23:12:44.690182-08:00","close_reason":"Test files written for turn machine, phrase chunker, and rolling window. May need iteration to pass.","labels":["backend","testing"],"dependencies":[{"issue_id":"vc-01e.1","depends_on_id":"vc-01e","type":"parent-child","created_at":"2026-02-05T17:26:09.034748-08:00","created_by":"Trevor"},{"issue_id":"vc-01e.1","depends_on_id":"vc-old.2","type":"blocks","created_at":"2026-02-05T17:26:09.037922-08:00","created_by":"Trevor"},{"issue_id":"vc-01e.1","depends_on_id":"vc-jlc.1","type":"blocks","created_at":"2026-02-05T17:26:09.040168-08:00","created_by":"Trevor"},{"issue_id":"vc-01e.1","depends_on_id":"vc-lwa.2","type":"blocks","created_at":"2026-02-05T17:26:09.042363-08:00","created_by":"Trevor"}]}
{"id":"vc-01e.2","title":"WebSocket integration tests: full conversation round-trip","description":"## What\nWrite integration tests that exercise the full WebSocket conversation flow with mocked STT/TTS/LLM backends.\n\n## Test Scenarios\n\n### Happy Path: Full Voice Turn\n1. Connect WebSocket\n2. Send binary audio frame\n3. Assert: receive transcript_partial messages\n4. Assert: receive transcript_final\n5. Send: transcript_send (or wait for auto-send)\n6. Assert: receive llm_token messages\n7. Assert: receive llm_done\n8. Assert: receive tts_meta + binary audio frames\n9. Assert: receive tts_done\n10. Assert: turn state returns to idle\n\n### Barge-In During Speaking\n1. Start full voice turn (get to SPEAKING state)\n2. Send barge_in message\n3. Assert: tts_done received\n4. Assert: turn state transitions to idle\n\n### Cancel During Thinking\n1. Start turn, send transcript\n2. Send cancel during THINKING\n3. Assert: llm_done with partial text\n4. Assert: turn state returns to idle\n\n### Text Input (No Voice)\n1. Connect WebSocket\n2. Send transcript_send directly (skip audio)\n3. Assert: full LLM + TTS pipeline runs\n\n### Slash Command\n1. Send command message\n2. Assert: receive command_result\n3. Assert: no LLM call made\n\n### Connection Recovery\n1. Connect, start a turn\n2. Forcibly close connection\n3. Reconnect\n4. Assert: clean state (no orphaned turns)\n\n## Mocking Strategy\n- Mock Parakeet HTTP: return canned transcripts\n- Mock OpenClaw Gateway WebSocket: return canned LLM responses\n- Mock Kokoro/OpenAI TTS: return canned audio buffers\n- Use real Fastify server with real WebSocket (in-process)\n\n## Acceptance Criteria\n- [ ] All 6 test scenarios pass\n- [ ] Tests use mocked backends (no external dependencies)\n- [ ] Tests complete in \u003c15s\n- [ ] Clear failure messages on assertion failures","status":"closed","priority":3,"issue_type":"task","owner":"trevor.j.atkinson@gmail.com","created_at":"2026-02-05T17:26:09.206191-08:00","created_by":"Trevor","updated_at":"2026-02-06T06:58:17.254446Z","closed_at":"2026-02-06T06:58:17.254365Z","close_reason":"Completed: WebSocket integration tests — ping/pong, commands, config, audio, turn state, error handling. All 6 tests pass in ~115ms.","labels":["integration","testing"],"dependencies":[{"issue_id":"vc-01e.2","depends_on_id":"vc-01e","type":"parent-child","created_at":"2026-02-05T17:26:09.207404-08:00","created_by":"Trevor"},{"issue_id":"vc-01e.2","depends_on_id":"vc-vej.7","type":"blocks","created_at":"2026-02-05T17:26:09.210189-08:00","created_by":"Trevor"},{"issue_id":"vc-01e.2","depends_on_id":"vc-jlc.4","type":"blocks","created_at":"2026-02-05T17:26:09.212552-08:00","created_by":"Trevor"},{"issue_id":"vc-01e.2","depends_on_id":"vc-ar0.2","type":"blocks","created_at":"2026-02-05T17:26:09.214846-08:00","created_by":"Trevor"}]}
{"id":"vc-01e.3","title":"Deployment: PM2 config, Caddy reverse proxy, systemd service","description":"## What\nSet up deployment configuration for running the Voice Gateway and web client in production.\n\n## Deployment Architecture\n```\nCaddy (reverse proxy, TLS)\n├── /          → Expo Web static files (or dev server)\n├── /ws        → Voice Gateway WebSocket (port 8788)\n├── /api/*     → Voice Gateway HTTP endpoints (port 8788)\n└── /gateway   → OpenClaw Gateway (port 18789)\n```\n\n## Configuration Files\n\n### PM2 (ecosystem.config.js)\n```javascript\nmodule.exports = {\n  apps: [\n    {\n      name: 'voice-gateway',\n      script: 'packages/gateway/dist/server.js',\n      env: {\n        PORT: 8788,\n        NODE_ENV: 'production',\n        PARAKEET_URL: 'http://100.86.69.14:8765',\n        KOKORO_URL: 'http://100.86.69.14:8787',\n      },\n      watch: false,\n      instances: 1,\n      max_memory_restart: '512M',\n    }\n  ]\n};\n```\n\n### Caddyfile\nUpdate existing Caddyfile to route to new gateway while keeping bridge.js routes for migration.\n\n### Build Script\n```bash\n# packages/gateway: TypeScript → JavaScript\ncd packages/gateway \u0026\u0026 npm run build\n# packages/client: Expo Web export\ncd packages/client \u0026\u0026 npx expo export --platform web\n```\n\n## Acceptance Criteria\n- [ ] PM2 config starts gateway in production mode\n- [ ] Caddy routes /ws and /api to gateway\n- [ ] Caddy serves Expo Web static files at /\n- [ ] Build script compiles both packages\n- [ ] Can run alongside existing bridge.js (different ports)","status":"closed","priority":3,"issue_type":"task","owner":"trevor.j.atkinson@gmail.com","created_at":"2026-02-05T17:26:09.378306-08:00","created_by":"Trevor","updated_at":"2026-02-06T07:00:34.286872Z","closed_at":"2026-02-06T07:00:34.28672Z","close_reason":"Completed: PM2 config, Caddy routing, build/deploy scripts","labels":["deployment","infrastructure"],"dependencies":[{"issue_id":"vc-01e.3","depends_on_id":"vc-01e","type":"parent-child","created_at":"2026-02-05T17:26:09.379514-08:00","created_by":"Trevor"},{"issue_id":"vc-01e.3","depends_on_id":"vc-1iq.2","type":"blocks","created_at":"2026-02-05T17:26:09.382331-08:00","created_by":"Trevor"},{"issue_id":"vc-01e.3","depends_on_id":"vc-vej.1","type":"blocks","created_at":"2026-02-05T17:26:09.384593-08:00","created_by":"Trevor"},{"issue_id":"vc-01e.3","depends_on_id":"vc-01e.2","type":"blocks","created_at":"2026-02-05T22:28:46.954191-08:00","created_by":"Trevor"}]}
{"id":"vc-1iq","title":"Epic: Project Scaffolding \u0026 Backend Foundation","description":"Set up the monorepo structure, Fastify backend, and core infrastructure that everything else depends on. This epic must complete before any feature work begins.\n\n## Background\nThe current V0 is a single bridge.js (Node HTTP) + index.html (vanilla JS). The target architecture is a Fastify+TypeScript backend (Voice Gateway) with a React Native/Expo frontend. This epic bootstraps both projects and establishes patterns.\n\n## Round 3 Feedback Integration\n- Per Round 3 #3: The Voice Gateway REPLACES bridge.js. Migration path: bridge.js stays running during development as the 'working reference', Voice Gateway is built alongside it, then cuts over.\n- Per Round 3 #7: Audio format is WAV (16kHz, 16-bit, mono) for MVP. No Opus encoding needed yet.\n- Per Round 3 #2: Web-first is fine for Phase 1. React Native is Phase 2 (if needed).","status":"open","priority":0,"issue_type":"epic","owner":"trevor.j.atkinson@gmail.com","created_at":"2026-02-05T17:26:03.380271-08:00","created_by":"Trevor","updated_at":"2026-02-05T17:26:03.380271-08:00","labels":["epic","foundation"]}
{"id":"vc-1iq.1","title":"Monorepo structure \u0026 tooling setup","description":"## What\nCreate the monorepo directory structure for the voice-chat project with separate packages for backend (gateway) and frontend (client).\n\n## Directory Structure\n```\nvoice-chat/\n├── packages/\n│   ├── gateway/          # Fastify backend (Voice Gateway)\n│   │   ├── src/\n│   │   │   ├── server.ts         # Fastify app setup, plugin registration\n│   │   │   ├── ws/               # WebSocket handler module\n│   │   │   ├── stt/              # STT router module\n│   │   │   ├── tts/              # TTS router module\n│   │   │   ├── llm/              # LLM/OpenClaw integration\n│   │   │   ├── session/          # Session management\n│   │   │   └── types.ts          # Shared TypeScript types\n│   │   ├── tsconfig.json\n│   │   └── package.json\n│   └── client/           # Web client (Expo Web for now)\n│       ├── app/          # Expo Router pages\n│       ├── components/\n│       ├── hooks/\n│       ├── stores/       # Zustand stores\n│       ├── lib/          # Utilities\n│       ├── app.json\n│       └── package.json\n├── package.json          # Root workspace config\n├── tsconfig.base.json    # Shared TS config\n└── .env.example          # Environment variable template\n```\n\n## Implementation Details\n- Use npm workspaces (not yarn/pnpm — keep it simple)\n- Root package.json with `workspaces: ['packages/*']`\n- Shared tsconfig.base.json with strict mode, ES2022 target\n- .env.example documenting all env vars from current bridge.js (PARAKEET_URL, OPENAI_API_KEY, KOKORO_URL, GATEWAY_URL, GATEWAY_TOKEN, etc.)\n- Copy existing bridge.js and index.html to project root as reference (don't delete them)\n\n## Acceptance Criteria\n- [ ] `npm install` from root installs all workspace dependencies\n- [ ] TypeScript compiles in both packages\n- [ ] .env.example has all environment variables documented\n- [ ] Existing bridge.js + index.html preserved as reference","status":"closed","priority":0,"issue_type":"task","owner":"trevor.j.atkinson@gmail.com","created_at":"2026-02-05T17:26:03.542042-08:00","created_by":"Trevor","updated_at":"2026-02-06T06:33:55.559961Z","closed_at":"2026-02-06T06:33:55.559826Z","close_reason":"Completed: monorepo structure with npm workspaces, gateway and client packages, TypeScript compiling in both","labels":["infrastructure","setup"],"dependencies":[{"issue_id":"vc-1iq.1","depends_on_id":"vc-1iq","type":"parent-child","created_at":"2026-02-05T17:26:03.543471-08:00","created_by":"Trevor"}]}
{"id":"vc-1iq.2","title":"Fastify server bootstrap with WebSocket support","description":"## What\nCreate the Fastify server with @fastify/websocket, pino logging, health endpoint, and graceful shutdown. This is the backbone of the Voice Gateway.\n\n## Implementation Details\n\n### Dependencies\n```json\n{\n  \"fastify\": \"^5.x\",\n  \"@fastify/websocket\": \"^11.x\",\n  \"@fastify/cors\": \"^10.x\",\n  \"better-sqlite3\": \"^11.x\",\n  \"pino\": \"(bundled with fastify)\",\n  \"dotenv\": \"^16.x\"\n}\n```\n\n### src/server.ts\n```typescript\nimport Fastify from 'fastify';\nimport websocket from '@fastify/websocket';\nimport cors from '@fastify/cors';\n\nconst app = Fastify({\n  logger: {\n    level: process.env.LOG_LEVEL || 'info',\n    transport: process.env.NODE_ENV === 'development' \n      ? { target: 'pino-pretty' } \n      : undefined\n  }\n});\n\n// Register plugins\nawait app.register(cors, { origin: true });\nawait app.register(websocket, {\n  options: {\n    maxPayload: 1024 * 1024 * 5 // 5MB max for audio chunks\n  }\n});\n\n// Health endpoint\napp.get('/health', async () =\u003e ({\n  ok: true,\n  uptime: process.uptime(),\n  timestamp: new Date().toISOString()\n}));\n\n// Graceful shutdown\nconst shutdown = async (signal: string) =\u003e {\n  app.log.info({ signal }, 'Shutting down...');\n  await app.close();\n  process.exit(0);\n};\nprocess.on('SIGINT', () =\u003e shutdown('SIGINT'));\nprocess.on('SIGTERM', () =\u003e shutdown('SIGTERM'));\n\nawait app.listen({ port: Number(process.env.PORT || 8788), host: '0.0.0.0' });\n```\n\n### Key Decisions\n- Port 8788 (not 8787 which is bridge.js) so both can run simultaneously during development\n- pino-pretty in dev, structured JSON in prod\n- CORS wide open for now (Tailscale network only)\n- 5MB max WebSocket payload (enough for ~30s of 16kHz WAV audio)\n\n## Acceptance Criteria\n- [ ] Server starts and responds to GET /health\n- [ ] WebSocket connections accepted at ws://host:8788\n- [ ] Structured JSON logging with request IDs\n- [ ] Graceful shutdown on SIGINT/SIGTERM\n- [ ] TypeScript compiles cleanly with strict mode","status":"closed","priority":0,"issue_type":"task","owner":"trevor.j.atkinson@gmail.com","created_at":"2026-02-05T17:26:03.703895-08:00","created_by":"Trevor","updated_at":"2026-02-06T06:38:09.294198Z","closed_at":"2026-02-06T06:38:09.294101Z","close_reason":"Completed: Fastify server with WebSocket, health endpoint, pino logging, graceful shutdown","labels":["backend","infrastructure"],"dependencies":[{"issue_id":"vc-1iq.2","depends_on_id":"vc-1iq","type":"parent-child","created_at":"2026-02-05T17:26:03.705277-08:00","created_by":"Trevor"},{"issue_id":"vc-1iq.2","depends_on_id":"vc-1iq.1","type":"blocks","created_at":"2026-02-05T17:26:03.707461-08:00","created_by":"Trevor"}]}
{"id":"vc-1iq.3","title":"Define shared TypeScript types for WebSocket protocol","description":"## What\nDefine all TypeScript types for the multiplexed WebSocket protocol. These types are the contract between client and server — they must be precise and shared.\n\n## Type Definitions (src/types.ts)\n\n### Turn States\n```typescript\nexport type TurnState = \n  | 'idle'          // Not listening, not processing\n  | 'listening'     // VAD detected speech, audio streaming\n  | 'transcribing'  // VAD endpoint, running STT\n  | 'pending_send'  // Transcript ready, user can edit before send\n  | 'thinking'      // Sent to LLM, tokens streaming\n  | 'speaking';     // TTS audio playing\n\n// State transitions (for documentation and validation)\nexport const VALID_TRANSITIONS: Record\u003cTurnState, TurnState[]\u003e = {\n  idle: ['listening'],\n  listening: ['transcribing', 'idle'],      // idle via barge-in cancel\n  transcribing: ['pending_send'],\n  pending_send: ['thinking', 'idle'],       // idle via cancel\n  thinking: ['speaking', 'idle'],           // idle via cancel\n  speaking: ['idle', 'listening'],          // listening via barge-in\n};\n```\n\n### Client → Server Messages\n```typescript\n// Binary frames are raw audio (16kHz, 16-bit, mono WAV chunks)\n// JSON frames use this union:\nexport type ClientMessage =\n  | { type: 'transcript_send'; text: string; turnId: string }\n  | { type: 'command'; name: string; args: string[] }\n  | { type: 'barge_in' }\n  | { type: 'cancel' }\n  | { type: 'config'; settings: Partial\u003cSessionConfig\u003e }\n  | { type: 'ping'; ts: number };\n```\n\n### Server → Client Messages\n```typescript\nexport type ServerMessage =\n  | { type: 'transcript_partial'; text: string; stable: string; unstable: string }\n  | { type: 'transcript_final'; text: string; turnId: string }\n  | { type: 'llm_token'; token: string; fullText: string }\n  | { type: 'llm_done'; fullText: string }\n  | { type: 'tts_meta'; format: 'wav'; index: number; sampleRate: number; durationMs: number }\n  // Binary frame follows tts_meta with raw audio bytes\n  | { type: 'tts_done' }\n  | { type: 'turn_state'; state: TurnState; turnId?: string }\n  | { type: 'error'; code: string; message: string; recoverable: boolean }\n  | { type: 'command_result'; name: string; result: unknown }\n  | { type: 'pong'; ts: number; serverTs: number };\n```\n\n### Session Config\n```typescript\nexport interface SessionConfig {\n  autoSendDelayMs: number;    // 0 = instant, 1500 = default\n  ttsProvider: 'kokoro' | 'openai';\n  ttsVoice: string;\n  sttProvider: 'parakeet' | 'cloud';\n  vadSensitivity: number;     // 0.0 - 1.0\n  llmModel: string;\n  agentId: string;\n  sessionKey: string;\n}\n```\n\n## Round 3 Integration\n- Per Change 2: TTS audio sent as binary frames preceded by tts_meta JSON frame (no base64)\n- Per Change 6: transcript_partial includes stable/unstable split for visual distinction\n- Per Change 8: expo-av replaced with expo-audio (noted for client-side types)\n\n## Acceptance Criteria\n- [ ] All message types defined with JSDoc comments explaining each field\n- [ ] TurnState enum with valid transitions documented\n- [ ] SessionConfig with defaults documented\n- [ ] Types exported for use by both gateway and client packages\n- [ ] No `any` types — everything fully typed","status":"closed","priority":0,"issue_type":"task","owner":"trevor.j.atkinson@gmail.com","created_at":"2026-02-05T17:26:03.868544-08:00","created_by":"Trevor","updated_at":"2026-02-06T06:36:54.8825Z","closed_at":"2026-02-06T06:36:54.882418Z","close_reason":"Completed: All WebSocket protocol types defined - TurnState with VALID_TRANSITIONS, ClientMessage (6 variants), ServerMessage (10 variants), SessionConfig with DEFAULT_CONFIG, plus TranscribeResult, PhraseChunk, VoiceCatalog. All JSDoc documented, no any types, tsc --noEmit passes clean.","labels":["protocol","shared","types"],"dependencies":[{"issue_id":"vc-1iq.3","depends_on_id":"vc-1iq","type":"parent-child","created_at":"2026-02-05T17:26:03.869852-08:00","created_by":"Trevor"},{"issue_id":"vc-1iq.3","depends_on_id":"vc-1iq.1","type":"blocks","created_at":"2026-02-05T17:26:03.87232-08:00","created_by":"Trevor"}]}
{"id":"vc-4ft","title":"Epic: Slash Commands","description":"Implement slash commands for quick runtime configuration. Commands are typed in the transcript/text input and intercepted before sending to LLM.\n\n## Commands\n| Command | Action |\n|---------|--------|\n| /model \u003cname\u003e | Switch LLM model |\n| /agent \u003cname\u003e | Switch agent/persona |\n| /voice \u003cname\u003e | Switch TTS voice |\n| /tts \u003cprovider\u003e | Switch TTS provider |\n| /stt \u003cprovider\u003e | Switch STT provider |\n| /clear | Clear conversation history |\n| /help | Show available commands |","status":"open","priority":2,"issue_type":"epic","owner":"trevor.j.atkinson@gmail.com","created_at":"2026-02-05T17:26:08.034709-08:00","created_by":"Trevor","updated_at":"2026-02-05T17:26:08.034709-08:00","labels":["commands","epic"]}
{"id":"vc-4ft.1","title":"Slash command parser and executor","description":"## What\nImplement a slash command parser that detects commands in user input and executes them.\n\n## Implementation\n\n### Client-Side Detection\nIn the transcript/text input, before sending transcript_send:\n1. Check if text starts with '/'\n2. Parse command name and args: `/command arg1 arg2`\n3. Send as `{ type: 'command', name: string, args: string[] }` instead of transcript_send\n4. Display command inline in chat as a system message\n\n### Server-Side Execution (src/ws/commands.ts)\n```typescript\nconst COMMANDS: Record\u003cstring, CommandHandler\u003e = {\n  model: async (args, conn) =\u003e {\n    const name = args[0];\n    if (!name) return { error: 'Usage: /model \u003cname\u003e' };\n    conn.config.llmModel = name;\n    return { message: `Switched to model: ${name}` };\n  },\n  voice: async (args, conn) =\u003e {\n    const name = args[0];\n    if (!name) return { error: 'Usage: /voice \u003cname\u003e' };\n    conn.config.ttsVoice = name;\n    return { message: `Switched to voice: ${name}` };\n  },\n  tts: async (args, conn) =\u003e {\n    const provider = args[0] as 'kokoro' | 'openai';\n    if (!['kokoro', 'openai'].includes(provider)) {\n      return { error: 'Usage: /tts kokoro|openai' };\n    }\n    conn.config.ttsProvider = provider;\n    return { message: `Switched to TTS: ${provider}` };\n  },\n  clear: async (args, conn) =\u003e {\n    // Clear conversation context (reset session)\n    return { message: 'Conversation cleared' };\n  },\n  help: async () =\u003e ({\n    message: 'Commands: /model \u003cname\u003e, /agent \u003cname\u003e, /voice \u003cname\u003e, /tts kokoro|openai, /stt parakeet|cloud, /clear, /help'\n  }),\n};\n```\n\n### Response\nServer sends `{ type: 'command_result', name: string, result: { message?: string, error?: string } }`\nClient displays result inline in chat as a system message (different styling from user/assistant).\n\n## Acceptance Criteria\n- [ ] Commands detected in user input (starts with /)\n- [ ] Parsed into name + args\n- [ ] Sent to server as command message (not transcript_send)\n- [ ] Server executes and returns result\n- [ ] Result displayed in chat as system message\n- [ ] /help lists all available commands\n- [ ] Unknown commands return helpful error","status":"closed","priority":2,"issue_type":"task","owner":"trevor.j.atkinson@gmail.com","created_at":"2026-02-05T17:26:08.197517-08:00","created_by":"Trevor","updated_at":"2026-02-06T06:53:08.96397Z","closed_at":"2026-02-06T06:53:08.963885Z","close_reason":"Completed: Slash command parser with 7 commands (model, agent, voice, tts, stt, clear, help), integrated into WS handler","labels":["backend","client","commands"],"dependencies":[{"issue_id":"vc-4ft.1","depends_on_id":"vc-4ft","type":"parent-child","created_at":"2026-02-05T17:26:08.198744-08:00","created_by":"Trevor"},{"issue_id":"vc-4ft.1","depends_on_id":"vc-old.1","type":"blocks","created_at":"2026-02-05T17:26:08.201719-08:00","created_by":"Trevor"},{"issue_id":"vc-4ft.1","depends_on_id":"vc-jlc.3","type":"blocks","created_at":"2026-02-05T17:26:08.203834-08:00","created_by":"Trevor"},{"issue_id":"vc-4ft.1","depends_on_id":"vc-1iq.3","type":"blocks","created_at":"2026-02-05T22:28:45.765705-08:00","created_by":"Trevor"},{"issue_id":"vc-4ft.1","depends_on_id":"vc-old.2","type":"blocks","created_at":"2026-02-05T22:28:45.88751-08:00","created_by":"Trevor"}]}
{"id":"vc-ar0","title":"Epic: LLM Integration via OpenClaw Gateway","description":"Connect to OpenClaw Gateway for LLM responses. Port the existing GatewayClient from bridge.js to TypeScript with streaming token delivery.\n\n## Background\nThe current bridge.js has a full GatewayClient class that handles WebSocket connection to OpenClaw, device authentication (Ed25519 signing), and chat message sending with streaming deltas. This needs to be ported to TypeScript and integrated with the new WebSocket-based architecture.\n\n## Key Points\n- OpenClaw Gateway uses its own WebSocket protocol (connect, connect.challenge, chat.send)\n- Device identity uses Ed25519 keypairs for signing\n- Chat responses come as streaming deltas via 'chat' events\n- The gateway client is a singleton — one connection shared across all voice chat clients","status":"open","priority":1,"issue_type":"epic","owner":"trevor.j.atkinson@gmail.com","created_at":"2026-02-05T17:26:05.190234-08:00","created_by":"Trevor","updated_at":"2026-02-05T17:26:05.190234-08:00","labels":["epic","gateway","llm"]}
{"id":"vc-ar0.1","title":"Port GatewayClient from bridge.js to TypeScript","description":"## What\nPort the GatewayClient class from bridge.js to TypeScript. This is the client that connects to OpenClaw Gateway WebSocket, handles authentication, and sends/receives chat messages.\n\n## Source Reference\nThe existing implementation is in bridge.js lines ~160-380. Key methods:\n- `ensureConnected()` — WebSocket connection with connect.challenge handshake\n- `buildConnectParams(challenge)` — Authentication params with device signing\n- `sendRequest(method, params)` — Generic RPC over WebSocket\n- `sendChat(sessionKey, message, { onDelta, onFinal })` — Chat with streaming\n- `handleChatEvent(payload)` — Process streaming chat events\n- `handleFrame(frame)` — Route incoming WebSocket frames\n\n## Implementation (src/llm/gateway-client.ts)\n\n### Key Typing Additions\n```typescript\ninterface GatewayFrame {\n  type: 'req' | 'res' | 'event';\n  id?: string;\n  method?: string;\n  params?: Record\u003cstring, unknown\u003e;\n  event?: string;\n  payload?: Record\u003cstring, unknown\u003e;\n  ok?: boolean;\n  error?: { message: string; code?: string };\n}\n\ninterface ChatCallbacks {\n  onDelta?: (text: string, payload: Record\u003cstring, unknown\u003e) =\u003e void;\n  onFinal?: (text: string, payload: Record\u003cstring, unknown\u003e) =\u003e void;\n}\n```\n\n### Changes from V0\n- Full TypeScript typing (no more `any`)\n- Use pino logger instead of console.log\n- Add reconnection with exponential backoff (1s, 2s, 4s, 8s, max 30s)\n- Add connection state events for health monitoring\n- Keep the extractText/mergeDeltaText helper functions — they handle OpenClaw's various response formats\n\n### Environment Variables (same as bridge.js)\n- GATEWAY_URL, GATEWAY_TOKEN, GATEWAY_PASSWORD\n- OPENCLAW_HOME, OPENCLAW_CONFIG_FILE\n- OPENCLAW_DEVICE_FILE, OPENCLAW_DEVICE_AUTH_FILE\n\n## Acceptance Criteria\n- [ ] Connects to OpenClaw Gateway WebSocket\n- [ ] Handles connect.challenge authentication flow\n- [ ] Device identity loaded/generated/saved (Ed25519)\n- [ ] sendChat() returns streaming deltas and final text\n- [ ] Auto-reconnect with exponential backoff\n- [ ] Full TypeScript — no any types\n- [ ] Passes integration test against real Gateway","status":"closed","priority":1,"issue_type":"task","owner":"trevor.j.atkinson@gmail.com","created_at":"2026-02-05T17:26:05.351014-08:00","created_by":"Trevor","updated_at":"2026-02-06T06:43:10.669812Z","closed_at":"2026-02-06T06:43:10.669731Z","close_reason":"Completed: GatewayClient ported to TypeScript with typed frames, reconnection with exponential backoff (1s/2s/4s/.../30s max), Ed25519 auth via tweetnacl, pino logger, connection state events, no any types","labels":["backend","llm","port"],"dependencies":[{"issue_id":"vc-ar0.1","depends_on_id":"vc-ar0","type":"parent-child","created_at":"2026-02-05T17:26:05.35232-08:00","created_by":"Trevor"},{"issue_id":"vc-ar0.1","depends_on_id":"vc-1iq.2","type":"blocks","created_at":"2026-02-05T17:26:05.361896-08:00","created_by":"Trevor"},{"issue_id":"vc-ar0.1","depends_on_id":"vc-1iq.3","type":"blocks","created_at":"2026-02-05T17:26:05.364362-08:00","created_by":"Trevor"}]}
{"id":"vc-ar0.2","title":"LLM pipeline: transcript → Gateway → streaming tokens → TTS","description":"## What\nWire the LLM into the turn pipeline: when PENDING_SEND transitions to THINKING (via auto-send or manual send), forward the transcript to OpenClaw Gateway and stream tokens back to the client.\n\n## Implementation (src/llm/pipeline.ts)\n\n### Pipeline Flow\n1. Turn machine transitions to THINKING\n2. Format message: prepend `[[voice]] Be brief.` prefix (same as V0)\n3. Call gateway.sendChat(sessionKey, message, { onDelta, onFinal })\n4. On each delta: send llm_token message to client via WebSocket\n5. On final: send llm_done message, begin TTS pipeline (see Epic 5)\n6. On error: send error message, transition to IDLE\n\n### Sentence-Level TTS Pipelining Trigger\nAs LLM tokens stream in, the pipeline should also feed them to the phrase chunker (see Epic 5). This means:\n- onDelta accumulates text in a buffer\n- Phrase chunker checks for sentence boundaries\n- Complete phrases are dispatched to TTS immediately (don't wait for llm_done)\n\n### Cancel Handling\nIf the client sends a `cancel` message during THINKING:\n1. Abort the gateway chat request (if possible — may need to track request IDs)\n2. Send llm_done with whatever text accumulated so far\n3. Transition to IDLE\n\n## Acceptance Criteria\n- [ ] Transcripts forwarded to Gateway with voice prefix\n- [ ] LLM tokens streamed to client in real-time\n- [ ] llm_done sent with full accumulated text\n- [ ] Errors handled gracefully with error message + IDLE transition\n- [ ] Cancel aborts in-flight request\n- [ ] Tokens also fed to phrase chunker for TTS pipelining","status":"closed","priority":1,"issue_type":"task","owner":"trevor.j.atkinson@gmail.com","created_at":"2026-02-05T17:26:05.526668-08:00","created_by":"Trevor","updated_at":"2026-02-06T06:46:36.083271Z","closed_at":"2026-02-06T06:46:36.08319Z","close_reason":"Completed: LLM pipeline with streaming tokens, phrase chunking for TTS, cancel support","labels":["backend","llm","pipeline"],"dependencies":[{"issue_id":"vc-ar0.2","depends_on_id":"vc-ar0","type":"parent-child","created_at":"2026-02-05T17:26:05.527954-08:00","created_by":"Trevor"},{"issue_id":"vc-ar0.2","depends_on_id":"vc-ar0.1","type":"blocks","created_at":"2026-02-05T17:26:05.530529-08:00","created_by":"Trevor"},{"issue_id":"vc-ar0.2","depends_on_id":"vc-old.2","type":"blocks","created_at":"2026-02-05T17:26:05.532265-08:00","created_by":"Trevor"}]}
{"id":"vc-eke","title":"Epic: Error Handling \u0026 Resilience","description":"Implement comprehensive error handling, graceful degradation, and resilience patterns across the stack.\n\n## Background\nThe spec calls for:\n- WebSocket reconnection with exponential backoff\n- STT/TTS provider fallback\n- LLM timeout handling with retry UI\n- Graceful degradation to text-only mode\n- Rate limiting\n\nSome of these are implemented in earlier epics (STT/TTS fallback). This epic covers the remaining gaps.","status":"open","priority":2,"issue_type":"epic","owner":"trevor.j.atkinson@gmail.com","created_at":"2026-02-05T17:26:08.366969-08:00","created_by":"Trevor","updated_at":"2026-02-05T17:26:08.366969-08:00","labels":["epic","errors","resilience"]}
{"id":"vc-eke.1","title":"Error recovery flows and graceful degradation to text-only","description":"## What\nImplement comprehensive error handling that keeps the app usable even when individual services fail.\n\n## Error Scenarios \u0026 Responses\n\n### WebSocket Connection Lost\n- Show 'Reconnecting...' banner at top of screen\n- Reconnect with exponential backoff (already in useWebSocket hook)\n- During disconnect: voice input disabled, text input still works (queues messages)\n- On reconnect: send any queued messages\n\n### STT Failure (All Providers)\n- Show 'Speech recognition unavailable' toast\n- Auto-switch to text-only input mode\n- Text input remains fully functional\n- Periodically retry STT (every 30s) and auto-restore\n\n### TTS Failure (All Providers)\n- Show 'Voice output unavailable' toast\n- LLM responses shown as text only (still perfectly usable)\n- Periodically retry TTS and auto-restore\n\n### LLM Timeout\n- If no tokens in 15s: show 'Still thinking...' with spinner\n- If no tokens in 30s: show 'Taking longer than usual' with Retry button\n- Don't auto-retry (LLM calls may have side effects via tool use)\n- Cancel button always available\n\n### Audio Permission Denied\n- Show clear explanation: 'Microphone access needed for voice chat'\n- Link/button to OS audio settings\n- Fall back to text-only mode\n\n### Corrupt Audio\n- Validate audio chunks: check for reasonable levels, non-zero data\n- Drop obviously corrupt chunks silently\n- Log for debugging\n\n## Implementation\nCreate an ErrorBoundary component and a useErrorRecovery hook that coordinates degradation across the app.\n\n## Acceptance Criteria\n- [ ] WebSocket disconnect shows reconnecting banner\n- [ ] Text input works during WebSocket disconnect\n- [ ] STT failure degrades to text-only input\n- [ ] TTS failure degrades to text-only output  \n- [ ] LLM timeout shows appropriate UI (15s / 30s thresholds)\n- [ ] Audio permission denied shows helpful message\n- [ ] All degradations are recoverable (auto-restore when service returns)","status":"closed","priority":2,"issue_type":"task","owner":"trevor.j.atkinson@gmail.com","created_at":"2026-02-05T17:26:08.533492-08:00","created_by":"Trevor","updated_at":"2026-02-06T06:58:06.824942Z","closed_at":"2026-02-06T06:58:06.82486Z","close_reason":"Completed: Error recovery with graceful degradation, auto-recovery, LLM timeout handling","labels":["client","errors","resilience"],"dependencies":[{"issue_id":"vc-eke.1","depends_on_id":"vc-eke","type":"parent-child","created_at":"2026-02-05T17:26:08.534718-08:00","created_by":"Trevor"},{"issue_id":"vc-eke.1","depends_on_id":"vc-vej.7","type":"blocks","created_at":"2026-02-05T17:26:08.537676-08:00","created_by":"Trevor"},{"issue_id":"vc-eke.1","depends_on_id":"vc-old.1","type":"blocks","created_at":"2026-02-05T22:28:46.008499-08:00","created_by":"Trevor"},{"issue_id":"vc-eke.1","depends_on_id":"vc-ar0.2","type":"blocks","created_at":"2026-02-05T22:28:46.129249-08:00","created_by":"Trevor"}]}
{"id":"vc-eke.2","title":"Client-side debounce and server-side rate limiting","description":"## What\nImplement rate limiting to prevent abuse and resource exhaustion.\n\n## Client-Side\n- Debounce rapid send attempts (no sending within 500ms of last send)\n- Don't send audio frames faster than the capture rate (shouldn't happen but defensive)\n- Debounce config changes (batch within 1s)\n\n## Server-Side\n- Per-connection rate limit on messages: 100/sec (configurable)\n- Per-connection rate limit on LLM calls: 30/min (configurable)\n- If limit exceeded: send error message with recoverable=true and cooldown info\n- Use sliding window counter (not token bucket — simpler for this use case)\n\n## Implementation (src/ws/rate-limiter.ts)\n```typescript\nclass SlidingWindowRateLimiter {\n  private timestamps: number[] = [];\n  \n  constructor(\n    private maxRequests: number,\n    private windowMs: number\n  ) {}\n\n  check(): boolean {\n    const now = Date.now();\n    this.timestamps = this.timestamps.filter(t =\u003e now - t \u003c this.windowMs);\n    if (this.timestamps.length \u003e= this.maxRequests) return false;\n    this.timestamps.push(now);\n    return true;\n  }\n}\n```\n\n## Acceptance Criteria\n- [ ] Client debounces rapid sends\n- [ ] Server enforces per-connection message rate limit\n- [ ] Server enforces per-connection LLM call rate limit\n- [ ] Rate limit exceeded returns recoverable error with cooldown info\n- [ ] Limits are configurable via env vars","status":"closed","priority":3,"issue_type":"task","owner":"trevor.j.atkinson@gmail.com","created_at":"2026-02-05T17:26:08.700188-08:00","created_by":"Trevor","updated_at":"2026-02-06T07:01:27.123498Z","closed_at":"2026-02-06T07:01:27.12335Z","close_reason":"Completed: Sliding window rate limiter, server integration, client debounce utility","labels":["backend","client","security"],"dependencies":[{"issue_id":"vc-eke.2","depends_on_id":"vc-eke","type":"parent-child","created_at":"2026-02-05T17:26:08.701373-08:00","created_by":"Trevor"},{"issue_id":"vc-eke.2","depends_on_id":"vc-old.1","type":"blocks","created_at":"2026-02-05T17:26:08.704334-08:00","created_by":"Trevor"},{"issue_id":"vc-eke.2","depends_on_id":"vc-eke.1","type":"blocks","created_at":"2026-02-05T22:28:46.24852-08:00","created_by":"Trevor"}]}
{"id":"vc-jlc","title":"Epic: Text-to-Speech Pipeline","description":"Implement the TTS pipeline with phrase-level chunking, Kokoro/OpenAI provider switching, and binary audio delivery over WebSocket.\n\n## Background\nCurrent V0 fetches TTS via HTTP (either Kokoro WAV or OpenAI streaming MP3). The new architecture:\n1. LLM tokens stream into a phrase chunker\n2. Each complete phrase is dispatched to TTS immediately\n3. TTS audio sent as binary WebSocket frames (preceded by tts_meta JSON)\n4. Client plays chunks in order as they arrive\n\n## Round 3 Integration\n- Change 2: Binary TTS frames (tts_meta JSON + binary audio, no base64)\n- Change 15: Phrase-aware chunker (not naive sentence split)\n- #6: TTS provider fallback (3 failures in 60s → auto-switch)","status":"open","priority":1,"issue_type":"epic","owner":"trevor.j.atkinson@gmail.com","created_at":"2026-02-05T17:26:05.692067-08:00","created_by":"Trevor","updated_at":"2026-02-05T17:26:05.692067-08:00","labels":["audio","epic","tts"]}
{"id":"vc-jlc.1","title":"Phrase-level text chunker for TTS pipelining","description":"## What\nImplement a phrase-aware text chunker that splits LLM output at natural boundaries for TTS dispatch. This is the critical latency optimization — the user hears the first sentence while the LLM is still generating.\n\n## Algorithm (src/tts/phrase-chunker.ts)\n\n### Rules (from Round 3, Change 15)\n1. Split at sentence boundaries (`.`, `!`, `?`) but NOT after common abbreviations (Mr., Dr., Mrs., e.g., i.e., etc., vs., Prof., Sr., Jr.)\n2. For long sentences (\u003e100 chars without a boundary), split at natural pause points: commas, semicolons, em-dashes (—), colons\n3. Minimum chunk size: 4 words (avoid choppy single-word TTS)\n4. Maximum chunk size: ~200 chars (avoid long TTS generation delays)\n5. Code blocks (```) and URLs are never split mid-token\n6. Preserve leading/trailing whitespace handling (trim before sending to TTS)\n\n### Implementation\n```typescript\nexport class PhraseChunker {\n  private buffer: string = '';\n  private chunkIndex: number = 0;\n  \n  // Common abbreviations that shouldn't trigger a split\n  private static readonly ABBREVIATIONS = new Set([\n    'mr.', 'mrs.', 'ms.', 'dr.', 'prof.', 'sr.', 'jr.',\n    'e.g.', 'i.e.', 'etc.', 'vs.', 'approx.', 'dept.',\n    'est.', 'inc.', 'ltd.', 'st.', 'ave.', 'blvd.'\n  ]);\n\n  feed(text: string, isFinal: boolean = false): PhraseChunk[] {\n    this.buffer += text;\n    const chunks: PhraseChunk[] = [];\n    \n    while (true) {\n      const split = this.findSplitPoint();\n      if (split === -1) break;\n      \n      const chunk = this.buffer.substring(0, split).trim();\n      this.buffer = this.buffer.substring(split);\n      \n      if (chunk \u0026\u0026 this.wordCount(chunk) \u003e= 4) {\n        chunks.push({ text: chunk, index: this.chunkIndex++ });\n      } else if (chunk) {\n        // Too short — prepend to buffer for next chunk\n        this.buffer = chunk + ' ' + this.buffer;\n      }\n    }\n\n    if (isFinal \u0026\u0026 this.buffer.trim()) {\n      chunks.push({ text: this.buffer.trim(), index: this.chunkIndex++ });\n      this.buffer = '';\n    }\n    \n    return chunks;\n  }\n\n  reset() {\n    this.buffer = '';\n    this.chunkIndex = 0;\n  }\n}\n```\n\n### Edge Cases\n- Code blocks: detect opening ``` and buffer until closing ``` (don't TTS code)\n- URLs: detect http(s):// and buffer until whitespace\n- Numbered lists: '1. ' is not a sentence end\n- Ellipsis: '...' is a sentence end\n- Emoji: don't split around emoji\n\n## Acceptance Criteria\n- [ ] Splits at sentence boundaries (. ! ?)\n- [ ] Respects abbreviation list (doesn't split after Dr., etc.)\n- [ ] Falls back to comma/semicolon/dash splits for long sentences\n- [ ] Minimum 4-word chunk size enforced\n- [ ] Maximum ~200 char chunk size\n- [ ] Code blocks and URLs not split\n- [ ] Final flush emits remaining buffer\n- [ ] Unit tests covering all edge cases","status":"closed","priority":1,"issue_type":"task","owner":"trevor.j.atkinson@gmail.com","created_at":"2026-02-05T17:26:05.85267-08:00","created_by":"Trevor","updated_at":"2026-02-06T06:40:15.449248Z","closed_at":"2026-02-06T06:40:15.449167Z","close_reason":"Completed: Phrase-level text chunker with abbreviation handling, code block detection, URL preservation","labels":["backend","text-processing","tts"],"dependencies":[{"issue_id":"vc-jlc.1","depends_on_id":"vc-jlc","type":"parent-child","created_at":"2026-02-05T17:26:05.853905-08:00","created_by":"Trevor"},{"issue_id":"vc-jlc.1","depends_on_id":"vc-1iq.3","type":"blocks","created_at":"2026-02-05T17:26:05.856415-08:00","created_by":"Trevor"}]}
{"id":"vc-jlc.2","title":"TTS provider clients: Kokoro (local) + OpenAI (cloud)","description":"## What\nCreate typed HTTP clients for both TTS providers: Kokoro (local MLX-audio) and OpenAI (cloud).\n\n## Kokoro Client (src/tts/kokoro-client.ts)\n\n### API\n- POST /api/tts: `{ text: string, voice: string }` → audio/wav\n- GET /api/voices: lists available voices by language\n- GET /health: health check\n\n### Implementation\n```typescript\nexport class KokoroClient {\n  private baseUrl: string; // http://100.86.69.14:8787\n  \n  async synthesize(text: string, voice: string = 'af_heart'): Promise\u003cBuffer\u003e {\n    const resp = await fetch(`${this.baseUrl}/api/tts`, {\n      method: 'POST',\n      headers: { 'Content-Type': 'application/json' },\n      body: JSON.stringify({ text, voice }),\n      signal: AbortSignal.timeout(10000),\n    });\n    if (!resp.ok) throw new TtsError(`Kokoro ${resp.status}`);\n    return Buffer.from(await resp.arrayBuffer());\n  }\n\n  async voices(): Promise\u003cVoiceCatalog\u003e { ... }\n  async healthCheck(): Promise\u003cboolean\u003e { ... }\n}\n```\n\n### Key Points\n- Default voice: af_heart (Round 3 #5)\n- Returns WAV audio (not streaming — Kokoro generates full audio then returns)\n- Timeout: 10s (Kokoro can take a few seconds for long text)\n\n## OpenAI Client (src/tts/openai-client.ts)\n\n### API\n- POST /v1/audio/speech: streaming audio generation\n\n### Implementation  \n```typescript\nexport class OpenAiTtsClient {\n  private apiKey: string;\n  \n  async synthesize(text: string, voice: string = 'cedar', options?: {\n    instructions?: string;\n    model?: string;\n  }): Promise\u003cBuffer\u003e {\n    const resp = await fetch('https://api.openai.com/v1/audio/speech', {\n      method: 'POST',\n      headers: {\n        'Authorization': `Bearer ${this.apiKey}`,\n        'Content-Type': 'application/json',\n      },\n      body: JSON.stringify({\n        model: options?.model || 'gpt-4o-mini-tts',\n        voice,\n        input: text,\n        instructions: options?.instructions,\n        response_format: 'wav',  // WAV for consistent format\n      }),\n    });\n    if (!resp.ok) throw new TtsError(`OpenAI TTS ${resp.status}`);\n    return Buffer.from(await resp.arrayBuffer());\n  }\n}\n```\n\n### Key Difference\n- Kokoro: returns WAV, local, free, ~54 voices\n- OpenAI: returns WAV (or mp3/opus), cloud, paid, ~12 voices, supports instructions\n\n## Acceptance Criteria\n- [ ] Kokoro client: synthesize, voices, healthCheck\n- [ ] OpenAI client: synthesize with instructions support  \n- [ ] Both return Buffer (WAV format) for consistent handling\n- [ ] Configurable timeouts\n- [ ] Custom TtsError class\n- [ ] Request timing logged","status":"closed","priority":1,"issue_type":"task","owner":"trevor.j.atkinson@gmail.com","created_at":"2026-02-05T17:26:06.017097-08:00","created_by":"Trevor","updated_at":"2026-02-06T06:43:35.669028Z","closed_at":"2026-02-06T06:43:35.668947Z","close_reason":"Completed: Kokoro + OpenAI TTS clients with WAV output, timeouts, health checks, request timing","labels":["backend","tts"],"dependencies":[{"issue_id":"vc-jlc.2","depends_on_id":"vc-jlc","type":"parent-child","created_at":"2026-02-05T17:26:06.018357-08:00","created_by":"Trevor"},{"issue_id":"vc-jlc.2","depends_on_id":"vc-1iq.2","type":"blocks","created_at":"2026-02-05T17:26:06.020714-08:00","created_by":"Trevor"},{"issue_id":"vc-jlc.2","depends_on_id":"vc-1iq.3","type":"blocks","created_at":"2026-02-05T22:28:30.287924-08:00","created_by":"Trevor"}]}
{"id":"vc-jlc.3","title":"TTS provider router with automatic fallback","description":"## What\nCreate a TTS router that dispatches to the configured provider with automatic fallback on failure.\n\n## Implementation (src/tts/router.ts)\n\n### Fallback Logic (Round 3 #6)\nIf the active TTS provider fails 3 consecutive times within 60 seconds:\n1. Auto-switch to the alternate provider\n2. Emit a 'provider_switched' event (client shows toast: 'Switched to [provider]')\n3. Manual switch back via /tts command\n\n### Router\n```typescript\nexport class TtsRouter {\n  private activeProvider: 'kokoro' | 'openai';\n  private kokoro: KokoroClient;\n  private openai: OpenAiTtsClient;\n  private failures: { timestamp: number }[] = [];\n  private readonly failureWindow = 60000; // 60s\n  private readonly failureThreshold = 3;\n\n  async synthesize(text: string, voice?: string): Promise\u003c{ audio: Buffer; provider: string }\u003e {\n    const client = this.activeProvider === 'kokoro' ? this.kokoro : this.openai;\n    try {\n      const audio = await client.synthesize(text, voice || this.defaultVoice());\n      this.failures = []; // Reset on success\n      return { audio, provider: this.activeProvider };\n    } catch (err) {\n      this.recordFailure();\n      if (this.shouldFallback()) {\n        this.switchProvider();\n        // Retry with alternate\n        const alt = this.activeProvider === 'kokoro' ? this.kokoro : this.openai;\n        const audio = await alt.synthesize(text, voice || this.defaultVoice());\n        return { audio, provider: this.activeProvider };\n      }\n      throw err;\n    }\n  }\n}\n```\n\n## Acceptance Criteria\n- [ ] Routes to configured provider by default\n- [ ] Falls back after 3 failures in 60s window\n- [ ] Emits event on provider switch\n- [ ] Manual provider switch via setProvider()\n- [ ] Reports active provider for status display","status":"closed","priority":1,"issue_type":"task","owner":"trevor.j.atkinson@gmail.com","created_at":"2026-02-05T17:26:06.181985-08:00","created_by":"Trevor","updated_at":"2026-02-06T06:46:26.173341Z","closed_at":"2026-02-06T06:46:26.173274Z","close_reason":"Completed: TTS router with automatic 3-failure fallback, manual override, health checks","labels":["backend","resilience","tts"],"dependencies":[{"issue_id":"vc-jlc.3","depends_on_id":"vc-jlc","type":"parent-child","created_at":"2026-02-05T17:26:06.183214-08:00","created_by":"Trevor"},{"issue_id":"vc-jlc.3","depends_on_id":"vc-jlc.2","type":"blocks","created_at":"2026-02-05T17:26:06.185812-08:00","created_by":"Trevor"}]}
{"id":"vc-jlc.4","title":"TTS pipeline: phrase chunks → TTS → binary WebSocket delivery","description":"## What\nWire the full TTS pipeline: phrase chunker feeds text chunks to TTS router, audio results sent as binary WebSocket frames with preceding tts_meta JSON frames.\n\n## Pipeline Flow (src/tts/pipeline.ts)\n\n1. LLM tokens stream into PhraseChunker\n2. PhraseChunker emits complete phrases\n3. Each phrase dispatched to TtsRouter.synthesize()\n4. Audio result sent to client:\n   a. Send tts_meta JSON: `{ type: 'tts_meta', format: 'wav', index: N, sampleRate: 16000, durationMs: X }`\n   b. Send binary frame: raw WAV audio bytes\n5. After all chunks processed: send `{ type: 'tts_done' }`\n6. Turn machine transitions SPEAKING → IDLE\n\n### Ordering\nChunks must be played in order. The `index` field in tts_meta tracks this. Client buffers if chunks arrive out of order (which shouldn't happen with sequential dispatch, but defensive coding).\n\n### Parallel TTS Requests\nFor speed, can dispatch up to 2 TTS requests in parallel (phrase N and phrase N+1). This hides network latency. But responses must be sent to client in order.\n\n### Binary Frame Format (Round 3 Change 2)\nThe spec originally had base64-encoded audio in JSON. Round 3 changed this to:\n- JSON frame: tts_meta with format, index, sampleRate, durationMs\n- Binary frame: immediately follows, contains raw audio bytes\n- Client matches binary frame to most recent tts_meta\n\n### Barge-In Handling\nIf client sends barge_in during SPEAKING:\n1. Cancel any in-flight TTS requests\n2. Discard queued but unsent chunks\n3. Send tts_done (even if not all chunks played)\n4. Transition to IDLE or LISTENING\n\n## Acceptance Criteria\n- [ ] Phrase chunks dispatched to TTS as they're available\n- [ ] Audio sent as tts_meta + binary frame pairs\n- [ ] Chunks indexed and ordered correctly\n- [ ] tts_done sent after all chunks or on barge-in\n- [ ] In-flight TTS requests cancellable\n- [ ] Up to 2 parallel TTS requests for latency hiding\n- [ ] Turn state transitions correctly on completion","status":"closed","priority":1,"issue_type":"task","owner":"trevor.j.atkinson@gmail.com","created_at":"2026-02-05T17:26:06.346245-08:00","created_by":"Trevor","updated_at":"2026-02-06T06:49:15.140263Z","closed_at":"2026-02-06T06:49:15.140182Z","close_reason":"Completed: TTS pipeline with ordered delivery, parallel synthesis, cancel support","labels":["backend","pipeline","tts"],"dependencies":[{"issue_id":"vc-jlc.4","depends_on_id":"vc-jlc","type":"parent-child","created_at":"2026-02-05T17:26:06.347468-08:00","created_by":"Trevor"},{"issue_id":"vc-jlc.4","depends_on_id":"vc-jlc.1","type":"blocks","created_at":"2026-02-05T17:26:06.350055-08:00","created_by":"Trevor"},{"issue_id":"vc-jlc.4","depends_on_id":"vc-jlc.3","type":"blocks","created_at":"2026-02-05T17:26:06.351848-08:00","created_by":"Trevor"},{"issue_id":"vc-jlc.4","depends_on_id":"vc-old.1","type":"blocks","created_at":"2026-02-05T17:26:06.353617-08:00","created_by":"Trevor"}]}
{"id":"vc-lwa","title":"Epic: Speech-to-Text Pipeline","description":"Implement the STT pipeline with rolling-window re-decode for Parakeet and cloud fallback for Deepgram.\n\n## Background\nCurrent V0 sends entire audio blob after VAD endpoint to Parakeet via HTTP POST. The new architecture streams audio chunks over WebSocket and uses rolling-window re-decode: every 300-700ms during LISTENING, send last 4-8s of audio to Parakeet, get full transcript, diff for stable/unstable prefix.\n\n## Round 3 Integration\n- Change 6: Rolling-window STT replaces batch transcription\n- The gateway maintains stablePrefix + unstableSuffix\n- Client renders stable text normally, unstable text in lighter color\n- On VAD end, one final full-audio decode produces transcript_final","status":"open","priority":1,"issue_type":"epic","owner":"trevor.j.atkinson@gmail.com","created_at":"2026-02-05T17:26:04.525136-08:00","created_by":"Trevor","updated_at":"2026-02-05T17:26:04.525136-08:00","labels":["audio","epic","stt"]}
{"id":"vc-lwa.1","title":"Parakeet STT HTTP client with retry/timeout","description":"## What\nCreate a typed HTTP client for the Parakeet-MLX STT server. Parakeet runs on Trevor's MacBook (100.86.69.14:8765) as a FastAPI server.\n\n## Implementation (src/stt/parakeet-client.ts)\n\n### API\nParakeet accepts: POST /transcribe with multipart form data containing an 'audio' file field (WAV format).\nReturns: `{ text: string, confidence: number, segments: Array\u003c{start: number, end: number, text: string}\u003e }`\n\n### Client\n```typescript\nexport class ParakeetClient {\n  private baseUrl: string;\n  private timeoutMs: number;\n\n  constructor(baseUrl: string = 'http://100.86.69.14:8765', timeoutMs: number = 5000) {\n    this.baseUrl = baseUrl;\n    this.timeoutMs = timeoutMs;\n  }\n\n  async transcribe(audioBuffer: Buffer, mimeType: string = 'audio/wav'): Promise\u003cTranscribeResult\u003e {\n    const form = new FormData();\n    const blob = new Blob([audioBuffer], { type: mimeType });\n    form.append('audio', blob, 'audio.wav');\n\n    const response = await fetch(`${this.baseUrl}/transcribe`, {\n      method: 'POST',\n      body: form,\n      signal: AbortSignal.timeout(this.timeoutMs),\n    });\n\n    if (!response.ok) {\n      throw new SttError(`Parakeet error ${response.status}`, response.status);\n    }\n\n    return response.json();\n  }\n\n  async healthCheck(): Promise\u003cboolean\u003e {\n    try {\n      const resp = await fetch(`${this.baseUrl}/health`, {\n        signal: AbortSignal.timeout(3000)\n      });\n      return resp.ok;\n    } catch { return false; }\n  }\n}\n```\n\n### Error Handling\n- Timeout: 5s per request (Parakeet typically responds in 200-500ms)\n- Health check: 3s timeout, used for fallback decision\n- Custom SttError class with status code for upstream handling\n- Log all request timings for latency tracking\n\n## Acceptance Criteria\n- [ ] Transcribes WAV audio buffer to text via Parakeet HTTP API\n- [ ] Configurable timeout (default 5s)\n- [ ] Health check endpoint for fallback routing\n- [ ] Custom error class with status codes\n- [ ] Request timing logged via pino","status":"closed","priority":1,"issue_type":"task","owner":"trevor.j.atkinson@gmail.com","created_at":"2026-02-05T17:26:04.692488-08:00","created_by":"Trevor","updated_at":"2026-02-06T06:42:57.719716Z","closed_at":"2026-02-06T06:42:57.719634Z","close_reason":"Completed: Parakeet STT client with timeout, health check, error handling","labels":["backend","stt"],"dependencies":[{"issue_id":"vc-lwa.1","depends_on_id":"vc-lwa","type":"parent-child","created_at":"2026-02-05T17:26:04.694085-08:00","created_by":"Trevor"},{"issue_id":"vc-lwa.1","depends_on_id":"vc-1iq.2","type":"blocks","created_at":"2026-02-05T17:26:04.696991-08:00","created_by":"Trevor"},{"issue_id":"vc-lwa.1","depends_on_id":"vc-1iq.3","type":"blocks","created_at":"2026-02-05T22:28:29.45787-08:00","created_by":"Trevor"}]}
{"id":"vc-lwa.2","title":"Rolling-window STT processor with stable/unstable prefix tracking","description":"## What\nImplement the rolling-window re-decode strategy for streaming STT. During LISTENING state, periodically send accumulated audio to Parakeet, track stable vs unstable text, and stream partial transcripts to client.\n\n## Algorithm (src/stt/rolling-window.ts)\n\n### Rolling Window Process\n1. Audio chunks accumulate in a buffer during LISTENING state\n2. Every 500ms (configurable, 300-700ms range), trigger a decode cycle\n3. Send the last 4-8 seconds of audio to Parakeet (not the full buffer — keeps latency constant)\n4. Compare new transcript with previous: text confirmed across 2+ consecutive decodes is 'stable'\n5. Send transcript_partial to client with { stable, unstable } split\n6. On VAD end (TRANSCRIBING state), send one final decode of the FULL audio\n7. Send transcript_final with complete text\n\n### Stable Prefix Algorithm\n```typescript\nclass RollingWindowSTT {\n  private decodeHistory: string[] = [];  // Last N decode results\n  private stablePrefix: string = '';\n  private intervalMs: number = 500;\n  private windowSec: number = 6;        // Audio window size\n  private stabilityThreshold: number = 2; // Decodes needed to confirm\n\n  processDecodeResult(transcript: string) {\n    this.decodeHistory.push(transcript);\n    if (this.decodeHistory.length \u003c this.stabilityThreshold) {\n      return { stable: this.stablePrefix, unstable: transcript };\n    }\n\n    // Find common prefix across last N decodes\n    const recent = this.decodeHistory.slice(-this.stabilityThreshold);\n    let commonPrefix = '';\n    for (let i = 0; i \u003c recent[0].length; i++) {\n      const char = recent[0][i];\n      if (recent.every(t =\u003e t[i] === char)) {\n        commonPrefix += char;\n      } else break;\n    }\n\n    // Snap to word boundary\n    const lastSpace = commonPrefix.lastIndexOf(' ');\n    if (lastSpace \u003e this.stablePrefix.length) {\n      this.stablePrefix = commonPrefix.substring(0, lastSpace + 1).trimEnd();\n    }\n\n    const unstable = transcript.substring(this.stablePrefix.length);\n    return { stable: this.stablePrefix, unstable };\n  }\n}\n```\n\n### WAV Construction for Window\nMust construct a valid WAV header for the windowed audio chunk:\n- Sample rate: 16000 Hz\n- Bit depth: 16-bit\n- Channels: 1 (mono)\n- Format: PCM (format code 1)\n\n### Timer Management\n- Start decode timer when entering LISTENING state\n- Clear timer when leaving LISTENING state\n- Don't send decode request if previous one is still in-flight (debounce)\n\n## Round 3 Integration (Change 6)\nThis is the direct implementation of the 'rolling-window re-decode' approach from Round 3 feedback. Key points:\n- Every 300-700ms, send last 4-8s of accumulated audio\n- Gateway maintains stablePrefix + unstableSuffix\n- Client renders stable text normally, unstable in lighter color/italic\n- On VAD end, one final full-audio decode\n\n## Acceptance Criteria\n- [ ] Periodic decode every 500ms during LISTENING\n- [ ] Audio windowed to last 6s (not full buffer)\n- [ ] Stable prefix tracked across consecutive decodes\n- [ ] transcript_partial messages sent with stable/unstable split\n- [ ] Final full-audio decode on VAD end\n- [ ] Timer properly cleared on state transitions\n- [ ] In-flight request debouncing (don't stack requests)","status":"closed","priority":1,"issue_type":"task","owner":"trevor.j.atkinson@gmail.com","created_at":"2026-02-05T17:26:04.858013-08:00","created_by":"Trevor","updated_at":"2026-02-06T06:46:30.016261Z","closed_at":"2026-02-06T06:46:30.016178Z","close_reason":"Completed: Rolling-window STT with stable prefix tracking, WAV construction, debounced decode cycles","labels":["backend","streaming","stt"],"dependencies":[{"issue_id":"vc-lwa.2","depends_on_id":"vc-lwa","type":"parent-child","created_at":"2026-02-05T17:26:04.859317-08:00","created_by":"Trevor"},{"issue_id":"vc-lwa.2","depends_on_id":"vc-lwa.1","type":"blocks","created_at":"2026-02-05T17:26:04.861921-08:00","created_by":"Trevor"},{"issue_id":"vc-lwa.2","depends_on_id":"vc-old.2","type":"blocks","created_at":"2026-02-05T17:26:04.863718-08:00","created_by":"Trevor"}]}
{"id":"vc-lwa.3","title":"STT provider fallback router (Parakeet → cloud)","description":"## What\nCreate an STT router that tries Parakeet first, falls back to a cloud STT provider if Parakeet is unreachable (timeout 3s), and falls back to Parakeet when it recovers.\n\n## Implementation (src/stt/router.ts)\n\n### Fallback Logic\n```typescript\nclass SttRouter {\n  private primary: ParakeetClient;\n  private fallback: CloudSttClient | null;\n  private usingFallback: boolean = false;\n  private healthCheckInterval: NodeJS.Timer | null = null;\n  private consecutiveFailures: number = 0;\n  private readonly failureThreshold = 3;\n\n  async transcribe(audio: Buffer): Promise\u003cTranscribeResult\u003e {\n    if (!this.usingFallback) {\n      try {\n        const result = await this.primary.transcribe(audio);\n        this.consecutiveFailures = 0;\n        return result;\n      } catch (err) {\n        this.consecutiveFailures++;\n        if (this.consecutiveFailures \u003e= this.failureThreshold) {\n          this.switchToFallback();\n        }\n        if (this.fallback) return this.fallback.transcribe(audio);\n        throw err;\n      }\n    }\n    // Using fallback — try it\n    return this.fallback!.transcribe(audio);\n  }\n\n  private switchToFallback() {\n    this.usingFallback = true;\n    // Start periodic health checks on primary\n    this.healthCheckInterval = setInterval(async () =\u003e {\n      if (await this.primary.healthCheck()) {\n        this.usingFallback = false;\n        this.consecutiveFailures = 0;\n        clearInterval(this.healthCheckInterval!);\n        this.healthCheckInterval = null;\n      }\n    }, 15000); // Check every 15s\n  }\n}\n```\n\n### Cloud STT (stub for MVP)\nFor MVP, the cloud fallback can be a stub that returns an error message ('Local STT unavailable'). Full Deepgram integration is a future enhancement. The router pattern should be in place regardless.\n\n## Acceptance Criteria\n- [ ] Routes to Parakeet by default\n- [ ] Switches to fallback after 3 consecutive failures\n- [ ] Periodically checks Parakeet health when using fallback\n- [ ] Auto-switches back when Parakeet recovers\n- [ ] Reports which provider is active (for status indicators)","status":"closed","priority":2,"issue_type":"task","owner":"trevor.j.atkinson@gmail.com","created_at":"2026-02-05T17:26:05.025262-08:00","created_by":"Trevor","updated_at":"2026-02-06T06:52:27.98731Z","closed_at":"2026-02-06T06:52:27.98723Z","close_reason":"Completed: STT router with 3-failure fallback, auto-recovery health checks, cloud stub","labels":["backend","resilience","stt"],"dependencies":[{"issue_id":"vc-lwa.3","depends_on_id":"vc-lwa","type":"parent-child","created_at":"2026-02-05T17:26:05.02655-08:00","created_by":"Trevor"},{"issue_id":"vc-lwa.3","depends_on_id":"vc-lwa.1","type":"blocks","created_at":"2026-02-05T17:26:05.029209-08:00","created_by":"Trevor"},{"issue_id":"vc-lwa.3","depends_on_id":"vc-lwa.2","type":"blocks","created_at":"2026-02-05T22:28:29.697015-08:00","created_by":"Trevor"}]}
{"id":"vc-old","title":"Epic: WebSocket Connection \u0026 Turn State Machine","description":"Implement the multiplexed WebSocket connection handler and the authoritative turn state machine on the server. This is the nervous system of the app — all audio, control messages, and state flow through it.\n\n## Background\nThe current bridge.js uses separate HTTP endpoints for STT, chat, and TTS. The new architecture uses a single WebSocket that multiplexes binary audio frames with JSON control messages. The server is authoritative on turn state; the client can optimistically transition but reconciles on turn_state messages.\n\n## Key Design Decisions\n- Binary frames = audio data (no JSON wrapping, no base64)\n- JSON frames = everything else (control, transcripts, LLM tokens, TTS metadata)\n- Server broadcasts turn_state changes; client renders accordingly\n- Each 'turn' gets a unique turnId for tracking through the pipeline","status":"open","priority":0,"issue_type":"epic","owner":"trevor.j.atkinson@gmail.com","created_at":"2026-02-05T17:26:04.031649-08:00","created_by":"Trevor","updated_at":"2026-02-05T17:26:04.031649-08:00","labels":["core","epic","websocket"]}
{"id":"vc-old.1","title":"WebSocket connection handler with binary/JSON multiplexing","description":"## What\nImplement the WebSocket route handler that accepts connections, identifies binary vs JSON frames, routes messages, and manages per-connection state.\n\n## Implementation Details\n\n### src/ws/handler.ts\n```typescript\nimport { FastifyInstance } from 'fastify';\nimport { WebSocket, RawData } from 'ws';\nimport { ClientMessage, ServerMessage, SessionConfig, TurnState } from '../types';\n\ninterface ConnectionState {\n  id: string;                    // Unique connection ID\n  ws: WebSocket;\n  config: SessionConfig;\n  turnState: TurnState;\n  turnId: string | null;\n  audioBuffer: Buffer[];         // Accumulated audio chunks for current turn\n  audioBufferBytes: number;      // Track buffer size\n  connectedAt: number;\n  lastPingAt: number;\n}\n\nexport function registerWebSocket(app: FastifyInstance) {\n  app.get('/ws', { websocket: true }, (socket, req) =\u003e {\n    const conn: ConnectionState = {\n      id: crypto.randomUUID(),\n      ws: socket,\n      config: DEFAULT_CONFIG,\n      turnState: 'idle',\n      turnId: null,\n      audioBuffer: [],\n      audioBufferBytes: 0,\n      connectedAt: Date.now(),\n      lastPingAt: Date.now(),\n    };\n\n    app.log.info({ connId: conn.id }, 'WebSocket connected');\n\n    socket.on('message', (data: RawData, isBinary: boolean) =\u003e {\n      if (isBinary) {\n        handleAudioFrame(conn, data as Buffer);\n      } else {\n        try {\n          const msg: ClientMessage = JSON.parse(data.toString());\n          handleJsonMessage(conn, msg);\n        } catch (err) {\n          sendMessage(conn, { type: 'error', code: 'PARSE_ERROR', message: 'Invalid JSON', recoverable: true });\n        }\n      }\n    });\n\n    socket.on('close', () =\u003e {\n      app.log.info({ connId: conn.id }, 'WebSocket disconnected');\n      cleanup(conn);\n    });\n\n    // Keepalive: ping every 30s, close if no pong in 10s\n    const keepalive = setInterval(() =\u003e {\n      if (socket.readyState !== WebSocket.OPEN) { clearInterval(keepalive); return; }\n      socket.ping();\n    }, 30000);\n\n    socket.on('pong', () =\u003e { conn.lastPingAt = Date.now(); });\n  });\n}\n```\n\n### Binary Frame Handling\nWhen a binary frame arrives during LISTENING state:\n1. Append to conn.audioBuffer\n2. Track total bytes (reject if \u003e 10MB accumulated)\n3. Forward to STT rolling-window processor (if streaming STT is active)\n\nWhen a binary frame arrives in any other state:\n- Log warning and discard (client shouldn't send audio outside LISTENING)\n\n### JSON Message Routing\n- `ping` → reply with `pong` (include server timestamp for latency measurement)\n- `transcript_send` → transition to THINKING, forward to LLM\n- `command` → route to command handler\n- `barge_in` → stop TTS, cancel LLM if still generating, transition to IDLE/LISTENING\n- `cancel` → abort current pipeline, transition to IDLE\n- `config` → update session config\n\n### Helper: sendMessage\n```typescript\nfunction sendMessage(conn: ConnectionState, msg: ServerMessage) {\n  if (conn.ws.readyState !== WebSocket.OPEN) return;\n  conn.ws.send(JSON.stringify(msg));\n}\n\nfunction sendBinary(conn: ConnectionState, data: Buffer) {\n  if (conn.ws.readyState !== WebSocket.OPEN) return;\n  conn.ws.send(data, { binary: true });\n}\n```\n\n## Acceptance Criteria\n- [ ] WebSocket accepts connections at /ws\n- [ ] Binary frames correctly identified and buffered\n- [ ] JSON frames parsed and routed by type\n- [ ] Per-connection state tracked (config, turn state, audio buffer)\n- [ ] Keepalive ping/pong every 30s\n- [ ] Clean disconnection handling (abort any in-flight pipeline)\n- [ ] Rate limiting: max 100 messages/sec per connection","status":"closed","priority":0,"issue_type":"task","owner":"trevor.j.atkinson@gmail.com","created_at":"2026-02-05T17:26:04.194722-08:00","created_by":"Trevor","updated_at":"2026-02-06T06:40:40.884629Z","closed_at":"2026-02-06T06:40:40.884547Z","close_reason":"Completed: WebSocket handler with binary/JSON multiplexing, connection state, keepalive, rate limiting","labels":["backend","core","websocket"],"dependencies":[{"issue_id":"vc-old.1","depends_on_id":"vc-old","type":"parent-child","created_at":"2026-02-05T17:26:04.196015-08:00","created_by":"Trevor"},{"issue_id":"vc-old.1","depends_on_id":"vc-1iq.2","type":"blocks","created_at":"2026-02-05T17:26:04.19848-08:00","created_by":"Trevor"},{"issue_id":"vc-old.1","depends_on_id":"vc-1iq.3","type":"blocks","created_at":"2026-02-05T17:26:04.200108-08:00","created_by":"Trevor"}]}
{"id":"vc-old.2","title":"Server-side turn state machine with transition validation","description":"## What\nImplement the finite state machine that governs turn progression. The server is authoritative — it validates transitions, rejects invalid ones, and broadcasts state changes to the client.\n\n## State Machine\n\n```\nIDLE ──(audio frame)──\u003e LISTENING ──(silence detected)──\u003e TRANSCRIBING\n  ^                        │                                   │\n  │                   (barge-in)                                │\n  │                        v                                   v\n  │                     IDLE ◄──(cancel)                PENDING_SEND\n  │                                                         │\n  │                                              (auto/manual send)\n  │                                                         │\n  │                                                         v\n  │                                                     THINKING\n  │                                                         │\n  │                                                         v\n  │                                                     SPEAKING\n  │                                                         │\n  │                           (TTS done / barge-in)         │\n  └─────────────────────────────────────────────────────────┘\n```\n\n## Implementation (src/ws/turn-machine.ts)\n\n```typescript\nimport { TurnState, VALID_TRANSITIONS } from '../types';\nimport { EventEmitter } from 'events';\n\nexport class TurnMachine extends EventEmitter {\n  private state: TurnState = 'idle';\n  private turnId: string | null = null;\n  private autoSendTimer: NodeJS.Timeout | null = null;\n  private autoSendDelayMs: number = 1500;\n\n  get currentState(): TurnState { return this.state; }\n  get currentTurnId(): string | null { return this.turnId; }\n\n  transition(to: TurnState, turnId?: string): boolean {\n    const validTargets = VALID_TRANSITIONS[this.state];\n    if (!validTargets.includes(to)) {\n      this.emit('invalid_transition', { from: this.state, to, turnId });\n      return false;\n    }\n    \n    const from = this.state;\n    this.state = to;\n    if (turnId) this.turnId = turnId;\n    \n    // Clear auto-send timer on any transition away from pending_send\n    if (from === 'pending_send' \u0026\u0026 to !== 'thinking') {\n      this.clearAutoSend();\n    }\n    \n    this.emit('transition', { from, to, turnId: this.turnId });\n    return true;\n  }\n\n  startAutoSend(delayMs?: number) {\n    this.clearAutoSend();\n    if (this.state !== 'pending_send') return;\n    const delay = delayMs ?? this.autoSendDelayMs;\n    if (delay \u003c= 0) {\n      // Instant send\n      this.emit('auto_send', { turnId: this.turnId });\n      return;\n    }\n    this.autoSendTimer = setTimeout(() =\u003e {\n      if (this.state === 'pending_send') {\n        this.emit('auto_send', { turnId: this.turnId });\n      }\n    }, delay);\n  }\n\n  reset() {\n    this.clearAutoSend();\n    this.state = 'idle';\n    this.turnId = null;\n  }\n\n  private clearAutoSend() {\n    if (this.autoSendTimer) {\n      clearTimeout(this.autoSendTimer);\n      this.autoSendTimer = null;\n    }\n  }\n}\n```\n\n## PENDING_SEND State Details\nThis is the key UX innovation from the spec. After STT produces a final transcript:\n1. Server transitions to PENDING_SEND\n2. Client shows transcript in an editable message box\n3. Auto-send timer starts (configurable, default 1.5s)\n4. User can:\n   - Edit the transcript and manually send → THINKING\n   - Let auto-send fire → THINKING\n   - Cancel → IDLE\n   - Speak again → restart to LISTENING (barge-in on own pending message)\n\n## Echo-Aware Barge-In (Round 3 Change 5)\nDuring SPEAKING state, transition to LISTENING requires:\n- VAD active for ≥200ms sustained\n- Mic energy exceeds playback energy by 6dB margin\nThis prevents TTS audio leaking into mic from triggering false barge-ins.\nImplementation: the barge_in message from client should include `durationMs` and `energyDb` fields. Server validates before transitioning.\n\n## Acceptance Criteria\n- [ ] All valid transitions work correctly\n- [ ] Invalid transitions are rejected with error event\n- [ ] Auto-send timer fires after configurable delay\n- [ ] Auto-send timer cleared on manual send or cancel\n- [ ] State changes emit events for WebSocket broadcast\n- [ ] Barge-in during SPEAKING validates duration/energy thresholds\n- [ ] Each turn gets a unique turnId propagated through the pipeline","status":"closed","priority":1,"issue_type":"task","owner":"trevor.j.atkinson@gmail.com","created_at":"2026-02-05T17:26:04.36793-08:00","created_by":"Trevor","updated_at":"2026-02-06T06:40:52.943303Z","closed_at":"2026-02-06T06:40:52.943216Z","close_reason":"Completed: Turn state machine with transition validation, auto-send timer, event emission","labels":["backend","core","state-machine"],"dependencies":[{"issue_id":"vc-old.2","depends_on_id":"vc-old","type":"parent-child","created_at":"2026-02-05T17:26:04.369104-08:00","created_by":"Trevor"},{"issue_id":"vc-old.2","depends_on_id":"vc-1iq.3","type":"blocks","created_at":"2026-02-05T17:26:04.371327-08:00","created_by":"Trevor"},{"issue_id":"vc-old.2","depends_on_id":"vc-1iq.2","type":"blocks","created_at":"2026-02-05T22:28:29.218317-08:00","created_by":"Trevor"}]}
{"id":"vc-vej","title":"Epic: Web Client (Expo Web)","description":"Build the web client using Expo Web. This is Phase 1 — a web client that replaces index.html with a proper React app. React Native (iOS) is Phase 2.\n\n## Background\nRound 3 (#2) raised the question: is React Native needed? For Phase 1, Expo Web gives us a React app with the same component model that could later target iOS. If RN is never needed, Expo Web is still a solid choice (it's just React with a nice toolkit).\n\n## Architecture\n- Expo Router for navigation (chat screen + settings screen)\n- Zustand for state management (turn state + UI state + config)\n- WebSocket hook for server communication\n- vad-web (Silero WASM) for VAD\n- Web Audio API (AudioWorklet) for mic capture\n- Audio element for TTS playback","status":"open","priority":1,"issue_type":"epic","owner":"trevor.j.atkinson@gmail.com","created_at":"2026-02-05T17:26:06.517062-08:00","created_by":"Trevor","updated_at":"2026-02-05T17:26:06.517062-08:00","labels":["client","epic","web"]}
{"id":"vc-vej.1","title":"Expo Web project setup with Router and dependencies","description":"## What\nInitialize the Expo project in packages/client/ with web support, Expo Router, and all required dependencies.\n\n## Setup Steps\n1. `npx create-expo-app@latest packages/client --template blank-typescript`\n2. Install dependencies:\n   - expo-router (file-based routing)\n   - zustand (state management)\n   - @ricky0123/vad-web (VAD for web)\n   - expo-sqlite (conversation persistence — deferred per Round 3 #4)\n3. Configure for web:\n   - app.json: `{ \"web\": { \"bundler\": \"metro\" } }`\n   - Enable Expo Web in metro.config.js\n\n## File Structure\n```\npackages/client/\n├── app/\n│   ├── _layout.tsx         # Root layout with providers\n│   ├── index.tsx           # Main chat screen\n│   └── settings.tsx        # Settings screen\n├── components/\n│   ├── ChatMessage.tsx     # Message bubble component\n│   ├── ChatHistory.tsx     # Scrollable message list\n│   ├── TranscriptBox.tsx   # Editable transcript input\n│   ├── VoiceButton.tsx     # Mic toggle with waveform\n│   ├── StatusIndicator.tsx # Turn state display\n│   └── SettingsPanel.tsx   # Config UI\n├── hooks/\n│   ├── useWebSocket.ts     # WebSocket connection hook\n│   ├── useVAD.ts           # Voice activity detection hook\n│   ├── useAudioCapture.ts  # Mic capture hook\n│   └── useAudioPlayback.ts # TTS playback hook\n├── stores/\n│   ├── turnStore.ts        # Turn state machine (Zustand)\n│   ├── chatStore.ts        # Message history (Zustand)\n│   └── configStore.ts      # User preferences (Zustand)\n├── lib/\n│   ├── types.ts            # Shared types (imported from gateway)\n│   └── audio-utils.ts      # WAV encoding, audio helpers\n├── app.json\n├── metro.config.js\n└── package.json\n```\n\n## Acceptance Criteria\n- [ ] `npx expo start --web` serves the app\n- [ ] Expo Router navigates between chat and settings screens\n- [ ] Zustand stores initialized with default state\n- [ ] TypeScript compiles cleanly\n- [ ] All dependencies installed and importable","status":"closed","priority":1,"issue_type":"task","owner":"trevor.j.atkinson@gmail.com","created_at":"2026-02-05T17:26:06.678443-08:00","created_by":"Trevor","updated_at":"2026-02-06T06:38:15.551816Z","closed_at":"2026-02-06T06:38:15.551732Z","close_reason":"Completed: Expo Web project with Router, two screens (chat + settings), Zustand, all deps working. TypeScript compiles clean, web export succeeds.","labels":["client","setup"],"dependencies":[{"issue_id":"vc-vej.1","depends_on_id":"vc-vej","type":"parent-child","created_at":"2026-02-05T17:26:06.679678-08:00","created_by":"Trevor"},{"issue_id":"vc-vej.1","depends_on_id":"vc-1iq.1","type":"blocks","created_at":"2026-02-05T17:26:06.682048-08:00","created_by":"Trevor"}]}
{"id":"vc-vej.2","title":"useWebSocket hook: multiplexed binary/JSON WebSocket client","description":"## What\nCreate a React hook that manages the WebSocket connection to the Voice Gateway, handles reconnection, and provides typed send/receive methods.\n\n## Implementation (hooks/useWebSocket.ts)\n\n### Hook API\n```typescript\ninterface UseWebSocketReturn {\n  isConnected: boolean;\n  isReconnecting: boolean;\n  send: (msg: ClientMessage) =\u003e void;\n  sendBinary: (data: ArrayBuffer) =\u003e void;\n  disconnect: () =\u003e void;\n  reconnect: () =\u003e void;\n  latencyMs: number | null;\n}\n\nfunction useWebSocket(url: string, handlers: {\n  onMessage: (msg: ServerMessage) =\u003e void;\n  onBinary: (data: ArrayBuffer) =\u003e void;\n  onConnect: () =\u003e void;\n  onDisconnect: () =\u003e void;\n}): UseWebSocketReturn;\n```\n\n### Features\n1. **Auto-reconnect with exponential backoff**: 1s, 2s, 4s, 8s, max 30s\n2. **Binary/JSON demux**: check message type, route to appropriate handler\n3. **Ping/pong latency tracking**: send ping every 15s, measure round-trip\n4. **Connection state tracking**: isConnected, isReconnecting\n5. **Message type validation**: parse JSON, validate 'type' field exists\n6. **Buffering during reconnect**: queue messages during disconnect, send on reconnect (configurable)\n\n### Reconnection Logic\n```typescript\nconst RECONNECT_DELAYS = [1000, 2000, 4000, 8000, 16000, 30000];\nlet reconnectAttempt = 0;\n\nfunction scheduleReconnect() {\n  const delay = RECONNECT_DELAYS[Math.min(reconnectAttempt, RECONNECT_DELAYS.length - 1)];\n  setTimeout(() =\u003e {\n    reconnectAttempt++;\n    connect();\n  }, delay);\n}\n\n// Reset attempt counter on successful connection\nfunction onOpen() {\n  reconnectAttempt = 0;\n}\n```\n\n### Binary Detection\nWebSocket messages arrive as MessageEvent. Check:\n- `event.data instanceof ArrayBuffer` → binary (audio)\n- `typeof event.data === 'string'` → JSON\n\n## Acceptance Criteria\n- [ ] Connects to gateway WebSocket on mount\n- [ ] Auto-reconnects with exponential backoff on disconnect\n- [ ] Routes binary frames to onBinary handler\n- [ ] Routes JSON frames (parsed) to onMessage handler\n- [ ] Provides latency measurement via ping/pong\n- [ ] Cleans up connection on unmount\n- [ ] Handles rapid mount/unmount without leaked connections","status":"closed","priority":1,"issue_type":"task","owner":"trevor.j.atkinson@gmail.com","created_at":"2026-02-05T17:26:06.842721-08:00","created_by":"Trevor","updated_at":"2026-02-06T06:47:11.764182Z","closed_at":"2026-02-06T06:47:11.764102Z","close_reason":"Completed: useWebSocket hook with binary/JSON multiplexing, auto-reconnect, latency tracking","labels":["client","hook","websocket"],"dependencies":[{"issue_id":"vc-vej.2","depends_on_id":"vc-vej","type":"parent-child","created_at":"2026-02-05T17:26:06.843952-08:00","created_by":"Trevor"},{"issue_id":"vc-vej.2","depends_on_id":"vc-vej.1","type":"blocks","created_at":"2026-02-05T17:26:06.846616-08:00","created_by":"Trevor"},{"issue_id":"vc-vej.2","depends_on_id":"vc-1iq.3","type":"blocks","created_at":"2026-02-05T17:26:06.848433-08:00","created_by":"Trevor"}]}
{"id":"vc-vej.3","title":"Zustand turn state store with client-side state machine","description":"## What\nCreate a Zustand store that mirrors the server's turn state machine on the client. The client can optimistically transition but reconciles when it receives turn_state messages from the server.\n\n## Implementation (stores/turnStore.ts)\n\n```typescript\nimport { create } from 'zustand';\nimport { TurnState, VALID_TRANSITIONS } from '../lib/types';\n\ninterface TurnStore {\n  state: TurnState;\n  turnId: string | null;\n  transcript: string;           // Current transcript text\n  stableTranscript: string;     // Stable prefix from rolling STT\n  unstableTranscript: string;   // Unstable suffix\n  llmText: string;              // Accumulated LLM response\n  autoSendDelayMs: number;\n  autoSendTimerActive: boolean;\n  \n  // Actions\n  transition: (to: TurnState, turnId?: string) =\u003e boolean;\n  reconcile: (serverState: TurnState, turnId?: string) =\u003e void;\n  setTranscript: (text: string) =\u003e void;\n  setPartialTranscript: (stable: string, unstable: string) =\u003e void;\n  appendLlmToken: (token: string, fullText: string) =\u003e void;\n  reset: () =\u003e void;\n}\n\nexport const useTurnStore = create\u003cTurnStore\u003e((set, get) =\u003e ({\n  state: 'idle',\n  turnId: null,\n  transcript: '',\n  stableTranscript: '',\n  unstableTranscript: '',\n  llmText: '',\n  autoSendDelayMs: 1500,\n  autoSendTimerActive: false,\n\n  transition: (to, turnId) =\u003e {\n    const { state } = get();\n    if (!VALID_TRANSITIONS[state]?.includes(to)) return false;\n    set({ state: to, ...(turnId ? { turnId } : {}) });\n    return true;\n  },\n\n  reconcile: (serverState, turnId) =\u003e {\n    // Server is authoritative — force state\n    set({ state: serverState, ...(turnId ? { turnId } : {}) });\n  },\n\n  setPartialTranscript: (stable, unstable) =\u003e {\n    set({ stableTranscript: stable, unstableTranscript: unstable, transcript: stable + unstable });\n  },\n\n  appendLlmToken: (token, fullText) =\u003e {\n    set({ llmText: fullText });\n  },\n\n  reset: () =\u003e set({\n    state: 'idle', turnId: null, transcript: '', stableTranscript: '',\n    unstableTranscript: '', llmText: '', autoSendTimerActive: false,\n  }),\n}));\n```\n\n## Key Behavior\n- **Optimistic transitions**: client transitions immediately for responsive UI\n- **Server reconciliation**: turn_state message from server overrides client state\n- **Conflict resolution**: if client is in a different state than server, log the discrepancy and adopt server state\n\n## Acceptance Criteria\n- [ ] All turn states represented\n- [ ] Valid transitions enforced\n- [ ] Server reconciliation overrides client state\n- [ ] Transcript text tracked (partial + final)\n- [ ] LLM text accumulated from tokens\n- [ ] Auto-send delay configurable\n- [ ] Reset clears all state","status":"closed","priority":1,"issue_type":"task","owner":"trevor.j.atkinson@gmail.com","created_at":"2026-02-05T17:26:07.014358-08:00","created_by":"Trevor","updated_at":"2026-02-06T06:44:11.758431Z","closed_at":"2026-02-06T06:44:11.758348Z","close_reason":"Completed: Zustand stores (turn, chat, config) with client-side state machine and shared types","labels":["client","state","zustand"],"dependencies":[{"issue_id":"vc-vej.3","depends_on_id":"vc-vej","type":"parent-child","created_at":"2026-02-05T17:26:07.015604-08:00","created_by":"Trevor"},{"issue_id":"vc-vej.3","depends_on_id":"vc-vej.1","type":"blocks","created_at":"2026-02-05T17:26:07.01828-08:00","created_by":"Trevor"},{"issue_id":"vc-vej.3","depends_on_id":"vc-1iq.3","type":"blocks","created_at":"2026-02-05T17:26:07.020128-08:00","created_by":"Trevor"}]}
{"id":"vc-vej.4","title":"useAudioCapture hook: VAD + microphone capture → binary WebSocket","description":"## What\nCreate a React hook that captures microphone audio using Web Audio API, runs VAD using vad-web (Silero WASM), and streams binary audio frames to the Voice Gateway.\n\n## Implementation (hooks/useAudioCapture.ts)\n\n### Hook API\n```typescript\ninterface UseAudioCaptureReturn {\n  isCapturing: boolean;\n  isSpeaking: boolean;        // VAD detected speech\n  start: () =\u003e Promise\u003cvoid\u003e;\n  stop: () =\u003e void;\n  mute: () =\u003e void;\n  unmute: () =\u003e void;\n  isMuted: boolean;\n}\n\nfunction useAudioCapture(options: {\n  sendBinary: (data: ArrayBuffer) =\u003e void;\n  onSpeechStart: () =\u003e void;\n  onSpeechEnd: (audio: Float32Array) =\u003e void;\n  vadSensitivity?: number;\n}): UseAudioCaptureReturn;\n```\n\n### VAD Setup (using @ricky0123/vad-web)\n```typescript\nimport { MicVAD } from '@ricky0123/vad-web';\n\nconst vad = await MicVAD.new({\n  positiveSpeechThreshold: 0.85,\n  minSpeechFrames: 12,\n  preSpeechPadFrames: 2,\n  onSpeechStart: () =\u003e {\n    // Notify turn store: IDLE → LISTENING\n    options.onSpeechStart();\n  },\n  onSpeechEnd: (audio: Float32Array) =\u003e {\n    // Audio is Float32 PCM at 16kHz\n    // Convert to WAV and send as binary\n    const wavBuffer = float32ToWav(audio, 16000);\n    options.sendBinary(wavBuffer);\n    options.onSpeechEnd(audio);\n  },\n});\n```\n\n### Audio Format\n- Sample rate: 16000 Hz (for STT compatibility)\n- Bit depth: 16-bit PCM\n- Channels: 1 (mono)\n- Format: WAV (with header) — per Round 3 #7\n- Encoding: use the same float32ToWav function from current index.html\n\n### Echo Cancellation (Round 3 Change 5)\nFor web, use browser-native AEC:\n```typescript\nconst stream = await navigator.mediaDevices.getUserMedia({\n  audio: {\n    echoCancellation: true,     // Browser AEC\n    noiseSuppression: true,\n    autoGainControl: true,\n    sampleRate: 16000,\n  }\n});\n```\n\n### Barge-In Gating\nDuring SPEAKING state, VAD triggers need extra validation:\n- VAD active for ≥200ms sustained\n- This is tracked by onSpeechStart timing vs the barge_in send\n\n## Acceptance Criteria\n- [ ] Captures microphone audio at 16kHz\n- [ ] VAD detects speech start and end\n- [ ] Audio converted to WAV and sent as binary WebSocket frame\n- [ ] Browser AEC enabled (echoCancellation: true)\n- [ ] Mute/unmute controls\n- [ ] Clean cleanup on unmount (stop stream, destroy VAD)\n- [ ] Permission denied handled with clear error message","status":"closed","priority":1,"issue_type":"task","owner":"trevor.j.atkinson@gmail.com","created_at":"2026-02-05T17:26:07.181474-08:00","created_by":"Trevor","updated_at":"2026-02-06T06:50:38.739005Z","closed_at":"2026-02-06T06:50:38.738922Z","close_reason":"Completed: useAudioCapture with VAD, WAV encoding, AEC, mute controls","labels":["audio","client","hook","vad"],"dependencies":[{"issue_id":"vc-vej.4","depends_on_id":"vc-vej","type":"parent-child","created_at":"2026-02-05T17:26:07.182711-08:00","created_by":"Trevor"},{"issue_id":"vc-vej.4","depends_on_id":"vc-vej.1","type":"blocks","created_at":"2026-02-05T17:26:07.190213-08:00","created_by":"Trevor"},{"issue_id":"vc-vej.4","depends_on_id":"vc-vej.2","type":"blocks","created_at":"2026-02-05T17:26:07.19261-08:00","created_by":"Trevor"}]}
{"id":"vc-vej.5","title":"useAudioPlayback hook: queue-based TTS audio playback","description":"## What\nCreate a React hook that manages a queue of TTS audio chunks, plays them in order, and handles barge-in (stop playback).\n\n## Implementation (hooks/useAudioPlayback.ts)\n\n### Hook API\n```typescript\ninterface UseAudioPlaybackReturn {\n  isPlaying: boolean;\n  queueChunk: (meta: TtsMeta, audioData: ArrayBuffer) =\u003e void;\n  stop: () =\u003e void;           // Barge-in: stop immediately\n  setVolume: (vol: number) =\u003e void;\n}\n\nfunction useAudioPlayback(options: {\n  onPlaybackStart: () =\u003e void;\n  onPlaybackEnd: () =\u003e void;\n  onChunkPlayed: (index: number) =\u003e void;\n}): UseAudioPlaybackReturn;\n```\n\n### Playback Queue\n```typescript\ninterface QueueEntry {\n  meta: TtsMeta;     // { format, index, sampleRate, durationMs }\n  audioData: ArrayBuffer;\n}\n\nclass AudioPlaybackQueue {\n  private queue: QueueEntry[] = [];\n  private audioContext: AudioContext;\n  private isPlaying: boolean = false;\n  private currentSource: AudioBufferSourceNode | null = null;\n\n  enqueue(entry: QueueEntry) {\n    // Insert in order by index\n    const idx = this.queue.findIndex(e =\u003e e.meta.index \u003e entry.meta.index);\n    if (idx === -1) this.queue.push(entry);\n    else this.queue.splice(idx, 0, entry);\n    \n    if (!this.isPlaying) this.playNext();\n  }\n\n  private async playNext() {\n    if (this.queue.length === 0) {\n      this.isPlaying = false;\n      this.onEnd();\n      return;\n    }\n\n    this.isPlaying = true;\n    const entry = this.queue.shift()!;\n    \n    const audioBuffer = await this.audioContext.decodeAudioData(\n      entry.audioData.slice(0) // Clone because decodeAudioData detaches\n    );\n    \n    const source = this.audioContext.createBufferSource();\n    this.currentSource = source;\n    source.buffer = audioBuffer;\n    source.connect(this.audioContext.destination);\n    source.onended = () =\u003e {\n      this.currentSource = null;\n      this.onChunkPlayed(entry.meta.index);\n      this.playNext();\n    };\n    source.start(0);\n  }\n\n  stop() {\n    this.queue = [];\n    if (this.currentSource) {\n      this.currentSource.onended = null;\n      try { this.currentSource.stop(); } catch {}\n      this.currentSource = null;\n    }\n    this.isPlaying = false;\n  }\n}\n```\n\n### AudioContext Management\n- Create AudioContext on first user interaction (browser autoplay policy)\n- Resume if suspended\n- Share across the app (singleton)\n\n### tts_meta + Binary Pairing\nThe WebSocket delivers tts_meta (JSON) followed by a binary frame. The hook needs to:\n1. Receive tts_meta → store temporarily\n2. Receive next binary frame → pair with stored tts_meta → enqueue\n3. Clear stored tts_meta\n\n## Acceptance Criteria\n- [ ] Queues TTS audio chunks in index order\n- [ ] Plays chunks sequentially without gaps\n- [ ] stop() immediately halts playback and clears queue\n- [ ] Handles AudioContext autoplay policy (user gesture required)\n- [ ] Pairs tts_meta JSON with subsequent binary frame\n- [ ] Reports playback state (isPlaying, onEnd)","status":"closed","priority":1,"issue_type":"task","owner":"trevor.j.atkinson@gmail.com","created_at":"2026-02-05T17:26:07.354308-08:00","created_by":"Trevor","updated_at":"2026-02-06T06:50:28.963401Z","closed_at":"2026-02-06T06:50:28.96332Z","close_reason":"Completed: useAudioPlayback with queue, ordered playback, stop/barge-in, volume control","labels":["audio","client","hook","playback"],"dependencies":[{"issue_id":"vc-vej.5","depends_on_id":"vc-vej","type":"parent-child","created_at":"2026-02-05T17:26:07.355848-08:00","created_by":"Trevor"},{"issue_id":"vc-vej.5","depends_on_id":"vc-vej.1","type":"blocks","created_at":"2026-02-05T17:26:07.358933-08:00","created_by":"Trevor"},{"issue_id":"vc-vej.5","depends_on_id":"vc-vej.2","type":"blocks","created_at":"2026-02-05T17:26:07.360845-08:00","created_by":"Trevor"}]}
{"id":"vc-vej.6","title":"Chat UI: message list, transcript box, status indicators","description":"## What\nBuild the core chat UI components: message history list, editable transcript box (for PENDING_SEND), and status indicators showing the current turn state.\n\n## Components\n\n### ChatHistory.tsx\n- Scrollable list of messages (FlatList on native, div on web)\n- Message bubbles: user (right-aligned, blue-ish) and assistant (left-aligned, dark)\n- Auto-scroll to bottom on new messages\n- Shows streaming LLM tokens in real-time (assistant bubble updates as tokens arrive)\n- Prefix: 'You: ' for user, 'Clawd: ' for assistant (matching V0)\n\n### ChatMessage.tsx\n```typescript\ninterface ChatMessageProps {\n  role: 'user' | 'assistant';\n  text: string;\n  isStreaming?: boolean;    // Show typing indicator\n  timestamp?: number;\n}\n```\n\n### TranscriptBox.tsx\nThe key PENDING_SEND UI element:\n- Editable text input showing the STT transcript\n- During TRANSCRIBING: shows partial transcript with stable text (normal) + unstable text (lighter/italic)\n- During PENDING_SEND: fully editable, with Send button and auto-send countdown indicator\n- Auto-send timer visualization: subtle progress bar or countdown number\n- Cancel button to discard transcript\n\n```typescript\ninterface TranscriptBoxProps {\n  stableText: string;\n  unstableText: string;\n  isEditable: boolean;       // true in PENDING_SEND\n  autoSendCountdown: number; // seconds remaining, 0 if disabled\n  onSend: (text: string) =\u003e void;\n  onCancel: () =\u003e void;\n}\n```\n\n### StatusIndicator.tsx\nVisual state indicator matching V0's emoji system:\n- IDLE: ⚪ Idle\n- LISTENING: 🎤 Listening...\n- TRANSCRIBING: ✍️ Transcribing...\n- PENDING_SEND: 📝 Review \u0026 Send (3s...)\n- THINKING: 🧠 Thinking...\n- SPEAKING: 🔊 Speaking...\n\n### VoiceButton.tsx\n- Mic toggle button (start/stop listening)\n- Shows mini waveform visualization when listening\n- Red dot when recording\n- Mute/unmute control\n\n## Styling\n- Dark theme matching V0 (#0f1115 background, #151922 card, #e5e7eb text)\n- System font stack\n- Responsive: works on mobile and desktop widths\n- No external CSS framework — inline styles or StyleSheet.create\n\n## Acceptance Criteria\n- [ ] Message list renders user and assistant messages\n- [ ] Auto-scrolls to bottom on new messages\n- [ ] Streaming LLM tokens render in real-time\n- [ ] TranscriptBox shows stable/unstable text distinction\n- [ ] TranscriptBox editable in PENDING_SEND state\n- [ ] Auto-send countdown visible\n- [ ] Status indicator shows all 6 turn states\n- [ ] Dark theme consistent with V0\n- [ ] Responsive layout","status":"closed","priority":1,"issue_type":"task","owner":"trevor.j.atkinson@gmail.com","created_at":"2026-02-05T17:26:07.52449-08:00","created_by":"Trevor","updated_at":"2026-02-06T06:50:30.198221Z","closed_at":"2026-02-06T06:50:30.198131Z","close_reason":"Completed: Chat UI components - message list, transcript box, status indicator, voice button","labels":["client","components","ui"],"dependencies":[{"issue_id":"vc-vej.6","depends_on_id":"vc-vej","type":"parent-child","created_at":"2026-02-05T17:26:07.525708-08:00","created_by":"Trevor"},{"issue_id":"vc-vej.6","depends_on_id":"vc-vej.1","type":"blocks","created_at":"2026-02-05T17:26:07.528479-08:00","created_by":"Trevor"},{"issue_id":"vc-vej.6","depends_on_id":"vc-vej.3","type":"blocks","created_at":"2026-02-05T17:26:07.53042-08:00","created_by":"Trevor"},{"issue_id":"vc-vej.6","depends_on_id":"vc-vej.2","type":"blocks","created_at":"2026-02-05T22:28:45.168311-08:00","created_by":"Trevor"}]}
{"id":"vc-vej.7","title":"Main chat screen: wire hooks + components into working conversation flow","description":"## What\nCreate the main chat screen (app/index.tsx) that wires together all hooks and components into a working voice conversation flow.\n\n## Screen Layout\n```\n┌──────────────────────────┐\n│ 🎙️ Voice Chat    ⚙️     │  Header with settings gear\n├──────────────────────────┤\n│ ⚪ Idle                  │  Status indicator\n├──────────────────────────┤\n│                          │\n│ [Chat message history]   │  Scrollable message list\n│                          │\n│ You: Hello there         │\n│                          │\n│ Clawd: Hi! How can I...  │\n│                          │\n├──────────────────────────┤\n│ [Transcript box / Input] │  TranscriptBox or text input\n├──────────────────────────┤\n│ 🎤 [Start] [Stop] [Mute]│  Control buttons\n└──────────────────────────┘\n```\n\n## Wiring (app/index.tsx)\n\n### WebSocket Message Routing\n```typescript\nfunction ChatScreen() {\n  const turn = useTurnStore();\n  const chat = useChatStore();\n  \n  const ws = useWebSocket(GATEWAY_URL, {\n    onMessage: (msg) =\u003e {\n      switch (msg.type) {\n        case 'transcript_partial':\n          turn.setPartialTranscript(msg.stable, msg.unstable);\n          break;\n        case 'transcript_final':\n          turn.setTranscript(msg.text);\n          turn.reconcile('pending_send', msg.turnId);\n          break;\n        case 'llm_token':\n          turn.appendLlmToken(msg.token, msg.fullText);\n          break;\n        case 'llm_done':\n          chat.addMessage('assistant', msg.fullText);\n          break;\n        case 'tts_meta':\n          // Store for pairing with next binary frame\n          pendingTtsMeta.current = msg;\n          break;\n        case 'tts_done':\n          // TTS complete\n          break;\n        case 'turn_state':\n          turn.reconcile(msg.state, msg.turnId);\n          break;\n        case 'error':\n          // Show error toast\n          break;\n        case 'pong':\n          // Update latency display\n          break;\n      }\n    },\n    onBinary: (data) =\u003e {\n      if (pendingTtsMeta.current) {\n        playback.queueChunk(pendingTtsMeta.current, data);\n        pendingTtsMeta.current = null;\n      }\n    },\n    onConnect: () =\u003e { /* show connected status */ },\n    onDisconnect: () =\u003e { /* show reconnecting status */ },\n  });\n  \n  const capture = useAudioCapture({\n    sendBinary: ws.sendBinary,\n    onSpeechStart: () =\u003e turn.transition('listening'),\n    onSpeechEnd: () =\u003e turn.transition('transcribing'),\n  });\n  \n  const playback = useAudioPlayback({\n    onPlaybackStart: () =\u003e turn.transition('speaking'),\n    onPlaybackEnd: () =\u003e turn.transition('idle'),\n  });\n  \n  // ... render ChatHistory, TranscriptBox, controls\n}\n```\n\n### Text Input Mode\nUsers can also type messages (not just voice):\n- Text input at bottom (same as V0)\n- Send button or Enter key\n- Sends transcript_send message to server\n- Works alongside voice — typing while idle is fine\n\n### Dual Mode Toggle\n- Mic icon switches between voice mode and text mode\n- Voice mode: shows waveform + transcript box\n- Text mode: shows standard keyboard input\n\n## Acceptance Criteria\n- [ ] Full voice conversation flow: speak → see transcript → edit/send → see LLM response → hear TTS\n- [ ] Text input mode works independently\n- [ ] WebSocket messages correctly routed to stores/hooks\n- [ ] Turn state drives UI (button states, status display)\n- [ ] Connection loss shows 'Reconnecting...' status\n- [ ] Barge-in works (speak during TTS → stops playback)\n- [ ] Clean error handling with user-visible feedback","status":"closed","priority":1,"issue_type":"task","owner":"trevor.j.atkinson@gmail.com","created_at":"2026-02-05T17:26:07.693015-08:00","created_by":"Trevor","updated_at":"2026-02-06T06:53:02.552841Z","closed_at":"2026-02-06T06:53:02.552755Z","close_reason":"Completed: Main chat screen with full voice conversation flow, text input, barge-in, auto-send","labels":["client","integration","screen"],"dependencies":[{"issue_id":"vc-vej.7","depends_on_id":"vc-vej","type":"parent-child","created_at":"2026-02-05T17:26:07.694273-08:00","created_by":"Trevor"},{"issue_id":"vc-vej.7","depends_on_id":"vc-vej.2","type":"blocks","created_at":"2026-02-05T17:26:07.69709-08:00","created_by":"Trevor"},{"issue_id":"vc-vej.7","depends_on_id":"vc-vej.3","type":"blocks","created_at":"2026-02-05T17:26:07.699079-08:00","created_by":"Trevor"},{"issue_id":"vc-vej.7","depends_on_id":"vc-vej.4","type":"blocks","created_at":"2026-02-05T17:26:07.702021-08:00","created_by":"Trevor"},{"issue_id":"vc-vej.7","depends_on_id":"vc-vej.5","type":"blocks","created_at":"2026-02-05T17:26:07.704102-08:00","created_by":"Trevor"},{"issue_id":"vc-vej.7","depends_on_id":"vc-vej.6","type":"blocks","created_at":"2026-02-05T17:26:07.706246-08:00","created_by":"Trevor"}]}
{"id":"vc-vej.8","title":"Settings screen: TTS/STT provider, voice, auto-send timer, VAD sensitivity","description":"## What\nCreate the settings screen for configuring voice chat preferences. Accessible via the gear icon on the chat screen.\n\n## Settings\n\n### TTS Settings\n- **Provider**: Kokoro (local) / OpenAI (cloud) — dropdown\n- **Voice**: Dropdown populated from /api/tts/voices (or ws command)\n  - Kokoro: ~54 voices grouped by language (Round 3 #5)\n  - OpenAI: 12 voices (alloy, ash, ballad, cedar, coral, echo, fable, nova, onyx, sage, shimmer, verse)\n- **Instructions** (OpenAI only): Textarea for TTS style instructions\n  - Default: The calm/composed instructions from V0\n\n### STT Settings\n- **Provider**: Parakeet (local) / Cloud — dropdown (cloud is stub for MVP)\n\n### Turn Settings\n- **Auto-send delay**: Slider 0-10s (0 = instant send, default 1.5s)\n- **VAD sensitivity**: Slider 0.0-1.0 (maps to positiveSpeechThreshold)\n\n### Session\n- **Session key**: Text input (for OpenClaw session routing)\n\n### Storage\nSettings persisted to localStorage (web) / AsyncStorage (native).\nSent to server via config message on WebSocket connect and on change.\n\n## UI\n- Modal/drawer accessible from main screen\n- Save/Cancel buttons\n- Changes apply immediately (no page reload)\n\n## Acceptance Criteria\n- [ ] All settings configurable via UI\n- [ ] Settings persisted to localStorage\n- [ ] Config sent to server on change\n- [ ] Voice dropdown populated from provider's voice list\n- [ ] TTS instructions textarea shown only for OpenAI\n- [ ] Auto-send delay slider with real-time preview","status":"closed","priority":2,"issue_type":"task","owner":"trevor.j.atkinson@gmail.com","created_at":"2026-02-05T17:26:07.86757-08:00","created_by":"Trevor","updated_at":"2026-02-06T06:56:56.713933Z","closed_at":"2026-02-06T06:56:56.713852Z","close_reason":"Completed: Settings screen with TTS/STT provider pickers, voice text input, auto-send delay slider (0-10s), VAD sensitivity slider (0-1), LLM model input, session key input. localStorage persistence built into configStore. Dark theme matching chat screen.","labels":["client","settings","ui"],"dependencies":[{"issue_id":"vc-vej.8","depends_on_id":"vc-vej","type":"parent-child","created_at":"2026-02-05T17:26:07.868773-08:00","created_by":"Trevor"},{"issue_id":"vc-vej.8","depends_on_id":"vc-vej.1","type":"blocks","created_at":"2026-02-05T17:26:07.871598-08:00","created_by":"Trevor"},{"issue_id":"vc-vej.8","depends_on_id":"vc-vej.3","type":"blocks","created_at":"2026-02-05T17:26:07.873631-08:00","created_by":"Trevor"},{"issue_id":"vc-vej.8","depends_on_id":"vc-vej.7","type":"blocks","created_at":"2026-02-05T22:28:45.647246-08:00","created_by":"Trevor"}]}
