<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <title>Voice Chat</title>
  <style>
    :root{color-scheme:light dark;font-family:system-ui,-apple-system,Segoe UI,Roboto,sans-serif}
    body{margin:0;background:#0f1115;color:#e5e7eb;display:flex;min-height:100vh;align-items:center;justify-content:center}
    #app{width:min(720px,92vw);background:#151922;border-radius:16px;padding:20px 20px 16px;box-shadow:0 10px 30px rgba(0,0,0,.35)}
    h1{margin:0 0 12px;font-size:22px}
    #status{display:flex;align-items:center;gap:8px;margin-bottom:12px;font-weight:600}
    #indicator{font-size:18px}
    #chat{height:300px;overflow:auto;background:#0b0e14;border-radius:12px;padding:12px;display:flex;flex-direction:column;gap:10px}
    .msg{padding:8px 10px;border-radius:10px;line-height:1.35;white-space:pre-wrap}
    .msg.user{background:#1f2937;align-self:flex-end}
    .msg.assistant{background:#0f172a;align-self:flex-start}
    #controls{display:flex;gap:8px;margin:12px 0 6px;flex-wrap:wrap}
    #textRow{display:flex;gap:8px;margin:10px 0 0}
    #textInput{flex:1;padding:10px 12px;border-radius:10px;border:1px solid #334155;background:#0b0e14;color:#e5e7eb}
    #ttsInstructions{width:100%;min-height:70px;padding:8px 10px;border-radius:8px;border:1px solid #334155;background:#0b0e14;color:#e5e7eb;resize:vertical}
    button{padding:10px 14px;border-radius:10px;border:0;background:#2563eb;color:white;font-weight:600;cursor:pointer}
    button.secondary{background:#334155}
    button:disabled{opacity:.5;cursor:not-allowed}
    #hint{font-size:12px;opacity:.7}
  </style>
</head>
<body>
  <div id="app">
    <h1>üéôÔ∏è Voice Chat</h1>
    <div id="status"><span id="indicator">‚ö™</span><span id="status-text">Idle</span></div>
    <div id="chat"></div>
    <div id="textRow">
      <input id="textInput" type="text" placeholder="Type a message‚Ä¶">
      <button id="sendBtn" class="secondary">Send</button>
    </div>
    <div id="controls">
      <button id="startBtn">Start Conversation</button>
      <button id="stopBtn" class="secondary" disabled>Stop Conversation</button>
      <button id="endToggle" class="secondary">End: Auto</button>
      <button id="endTurnBtn" class="secondary" style="display:none" disabled>End Turn</button>
      <button id="muteBtn" class="secondary">Mute Mic</button>
      <button id="unlockBtn" class="secondary" style="display:none">Unlock Audio</button>
      <button id="speechToggle" class="secondary">Speech: On</button>
      <button id="configBtn" class="secondary">Config</button>
    </div>
    <!-- replyAudio removed -->
    <div id="hint">Mic permission required. Bridge server handles STT/LLM/TTS. <span id="build"></span></div>
  </div>

  <dialog id="configDialog">
    <form method="dialog" id="configForm" style="display:flex;flex-direction:column;gap:10px;min-width:320px">
      <h3 style="margin:0">Config</h3>
      <label>Session Key<input id="sessionKey" type="text" placeholder="main" style="width:100%"></label>
      <label>TTS Provider
        <select id="ttsProvider" style="width:100%;padding:6px 8px;border-radius:6px;border:1px solid #334155;background:#0b0e14;color:#e5e7eb">
          <option value="openai">OpenAI</option>
          <option value="kokoro">Kokoro (Local)</option>
        </select>
      </label>
      <label>TTS Voice<select id="voice" style="width:100%;padding:6px 8px;border-radius:6px;border:1px solid #334155;background:#0b0e14;color:#e5e7eb"></select></label>
      <label id="ttsInstructionsLabel">TTS Instructions<textarea id="ttsInstructions" placeholder="e.g., Speak warmly, calm pace, slight smile."></textarea></label>
      <div style="display:flex;gap:8px;justify-content:flex-end;margin-top:4px">
        <button id="cancelConfig" type="button" class="secondary">Cancel</button>
        <button id="saveConfig" type="submit">Save</button>
      </div>
    </form>
  </dialog>

  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.22.0/dist/ort.wasm.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@ricky0123/vad-web@0.0.29/dist/bundle.min.js"></script>
  <script>
    (() => {
      const BUILD = '2026-02-04-0919';
      const $ = id => document.getElementById(id),
        statusText = $('status-text'),
        indicator = $('indicator'),
        chat = $('chat'),
        textInput = $('textInput'),
        sendBtn = $('sendBtn'),
        startBtn = $('startBtn'),
        stopBtn = $('stopBtn'),
        endToggle = $('endToggle'),
        endTurnBtn = $('endTurnBtn'),
        muteBtn = $('muteBtn'),
        unlockBtn = $('unlockBtn'),
        speechToggle = $('speechToggle'),
        // replyAudioEl removed
        configBtn = $('configBtn'),
        configDialog = $('configDialog'),
        configForm = $('configForm'),
        cancelConfig = $('cancelConfig'),
        ttsInstructions = $('ttsInstructions');

      const buildEl = $('build');
      if (buildEl) buildEl.textContent = `build ${BUILD}`;

      const DEFAULT_TTS_INSTRUCTIONS = "Voice Affect: Calm, composed, and reassuring; project quiet authority and confidence.\n\nTone: Sincere, empathetic, and gently authoritative‚Äîexpress genuine apology while conveying competence.\n\nPacing: Steady and moderate; unhurried enough to communicate care, yet efficient enough to demonstrate professionalism.\n\nEmotion: Genuine empathy and understanding; speak with warmth, especially during apologies (\"I'm very sorry for any disruption...\").\n\nPronunciation: Clear and precise, emphasizing key reassurances (\"smoothly,\" \"quickly,\" \"promptly\") to reinforce confidence.\n\nPauses: Brief pauses after offering assistance or requesting details, highlighting willingness to listen and support.";
      const SAMPLE_RATE = 16000;
      // Base silence used for endpointing; adaptive logic stretches it for longer turns.
      const ENDPOINT_SILENCE_MS = 650;
      const ENDPOINT_MIN_SILENCE_MS = 450;
      const ENDPOINT_MAX_SILENCE_MS = 1600;
      const ENDPOINT_JOIN_SILENCE_MS = 120;
      const BARGE_IN_HOLDOFF_MS = 700;
      const BARGE_IN_CONFIRM_MS = 400;
      const BARGE_IN_MIN_MS = 350;
      // Disable chunked TTS; wait for the final reply and speak it as one piece.
      const STREAM_TTS_ON_DELTA = false;
      const TTS_CHUNK_MIN_CHARS = 18;
      const config = { sessionKey:'main', voice:'cedar', instructions: DEFAULT_TTS_INSTRUCTIONS, ttsProvider:'openai' };
      let status='stopped', running=false, starting=false, busy=false, vadInstance=null, currentAssistantEl=null, currentAudio=null, lastReplyText='', audioCtx=null, audioUnlocked=false, currentSource=null, isSpeaking=false, isTtsFetching=false, speechEnabled=true, isRecording=false, isMuted=false, autoEndpointing=true, lastAudioUrl=null, autoplayPrimed=false, autoplayPrimeAudio=null;
      const audioQueue = [];
      const ttsQueue = [];
      let ttsStreamState = null;
      let pendingSpeech = null;
      let pendingSpeechMeta = null;
      let endpointTimer = null;
      let adaptiveEndpointMs = ENDPOINT_SILENCE_MS;
      let lastTurnEndAt = 0;
      let lastTurnGapMs = null;
      let lastCommittedTurnId = null;
      let lastCommitAt = 0;
      let turnSeq = 0;
      let activeTurnId = 0;
      let currentTtsAbort = null;
      let currentTtsRequestId = 0;
      let lastTtsStartAt = 0;
      let bargeInTimer = null;
      let bargeInStartAt = 0;
      const statusMap = { listening:['üü¢','Listening...'], recording:['üî¥','Recording...'], processing:['üü°','Processing...'], thinking:['üü£','Thinking...'], speaking:['üîµ','Speaking...'], error:['‚ö´','Error'], stopped:['‚èπÔ∏è','Stopped'] };

      const setStatus = (state, detail) => {
        status = state;
        let s = state;
        if (isMuted && !['speaking','error','stopped'].includes(state)) s = 'stopped';
        const [icon, text] = statusMap[s] || ['‚ö™', s];
        indicator.textContent = icon;
        statusText.textContent = detail || (isMuted && !['speaking','error','stopped'].includes(state) ? 'Muted' : text);
      };

      const clamp = (value, min, max) => Math.min(max, Math.max(min, value));
      const computeEndpointMs = (speechMs=0) => {
        const base = ENDPOINT_SILENCE_MS;
        const speechBoost = Math.min(400, Math.round(speechMs * 0.15));
        const adaptive = adaptiveEndpointMs || base;
        return clamp(Math.max(base + speechBoost, adaptive), ENDPOINT_MIN_SILENCE_MS, ENDPOINT_MAX_SILENCE_MS);
      };
      const updateAdaptiveEndpoint = (transcript) => {
        const words = transcript.trim().split(/\s+/).filter(Boolean).length;
        if (!words) return;
        const extra = Math.min(600, words * 40);
        adaptiveEndpointMs = clamp(ENDPOINT_SILENCE_MS + extra, ENDPOINT_MIN_SILENCE_MS, ENDPOINT_MAX_SILENCE_MS);
      };
      const mergeAudioChunks = (chunks) => {
        if (!chunks || chunks.length === 0) return new Float32Array();
        if (chunks.length === 1) return chunks[0];
        const gapSamples = Math.max(0, Math.floor(SAMPLE_RATE * (ENDPOINT_JOIN_SILENCE_MS / 1000)));
        const total = chunks.reduce((sum, chunk, idx) => sum + chunk.length + (idx ? gapSamples : 0), 0);
        const merged = new Float32Array(total);
        let offset = 0;
        chunks.forEach((chunk, idx) => {
          if (idx) offset += gapSamples;
          merged.set(chunk, offset);
          offset += chunk.length;
        });
        return merged;
      };
      const clearPendingReplies = (reason) => {
        if (ttsQueue.length) {
          console.log(`[queue] drop ${ttsQueue.length} pending tts${reason ? ` (${reason})` : ''}`);
        }
        ttsQueue.length = 0;
        ttsStreamState = null;
      };
      const abortTtsFetch = (reason) => {
        if (!currentTtsAbort) return;
        try { currentTtsAbort.abort(); } catch {}
        currentTtsAbort = null;
        isTtsFetching = false;
        currentTtsRequestId += 1;
        if (reason) console.log(`[tts] abort${reason ? ` (${reason})` : ''}`);
      };
      const stopTtsPlayback = (reason) => {
        if (currentSource) {
          currentSource.onended = null;
          try { currentSource.stop(); } catch {}
          currentSource = null;
        }
        if (currentAudio) {
          try { currentAudio.pause(); } catch {}
          currentAudio.src = '';
          currentAudio = null;
        }
        if (isSpeaking) isSpeaking = false;
        if (reason) console.log(`[tts] stop${reason ? ` (${reason})` : ''}`);
      };
      const beginUserTurn = (source, { bargeIn=false } = {}) => {
        if (bargeIn) {
          stopTtsPlayback('barge-in');
          abortTtsFetch('barge-in');
        }
        clearPendingReplies(`new ${source} input`);
        const id = ++turnSeq;
        activeTurnId = id;
        return id;
      };
      const logTurnTiming = (timing, { ttsSkipped=false, ttsAborted=false } = {}) => {
        if (!timing || timing.logged) return;
        if (!timing.vadEndAt || !timing.transcribeEndAt || !timing.chatEndAt) return;
        const vadToTranscribe = timing.transcribeEndAt - timing.vadEndAt;
        const transcribeToChat = timing.chatEndAt - timing.transcribeEndAt;
        const ttsEndAt = timing.ttsEndAt;
        const chatToTts = ttsEndAt ? ttsEndAt - timing.chatEndAt : null;
        const total = (ttsEndAt || timing.chatEndAt) - timing.vadEndAt;
        const gapLabel = timing.gapMs != null ? `${Math.round(timing.gapMs)}ms` : 'n/a';
        const ttsLabel = ttsSkipped ? 'skipped' : ttsAborted ? 'aborted' : (chatToTts != null ? `${Math.round(chatToTts)}ms` : 'pending');
        console.log(`[timing] turn ${timing.turnId || '?'} gap ${gapLabel} vad->transcribe ${Math.round(vadToTranscribe)}ms transcribe->chat ${Math.round(transcribeToChat)}ms chat->tts ${ttsLabel} total ${Math.round(total)}ms`);
        timing.logged = true;
      };

      const tryPlayTts = async () => {
        if (!speechEnabled) return;
        try { await ensureAudioContext(); } catch {}
        audioUnlocked = audioCtx?.state === 'running';
        if (!audioUnlocked) {
          if (unlockBtn) unlockBtn.style.display = 'inline-block';
          return;
        }
        if (isSpeaking || isTtsFetching || isRecording) return;
        if (!ttsQueue.length) return;
        const next = ttsQueue.shift();
        await speak(next.text, next.timing || null);
      };
      const queueReply = (text, timing) => {
        if (!text) return;
        ttsQueue.push({ text, timing: timing || null });
      };
      const appendDeltaText = (prev, next) => {
        if (!next) return '';
        if (!prev) return next;
        if (next.startsWith(prev)) return next.slice(prev.length);
        if (prev.startsWith(next)) return '';
        const max = Math.min(prev.length, next.length);
        let i = 0;
        while (i < max && prev[i] === next[i]) i += 1;
        if (i === 0) return next;
        return next.slice(i);
      };
      const splitTtsChunks = (text, final=false) => {
        const chunks = [];
        let start = 0;
        for (let i = 0; i < text.length; i += 1) {
          const ch = text[i];
          if (ch === '.' || ch === '!' || ch === '?' || ch === '\n') {
            let end = i + 1;
            while (end < text.length && /\s/.test(text[end])) end += 1;
            chunks.push(text.slice(start, end));
            start = end;
            i = end - 1;
          }
        }
        let remaining = text.slice(start);
        if (!final && chunks.length) {
          const last = chunks[chunks.length - 1];
          if (last.trim().length < TTS_CHUNK_MIN_CHARS) {
            remaining = last + remaining;
            chunks.pop();
          }
        }
        if (final && remaining.trim()) {
          chunks.push(remaining);
          remaining = '';
        }
        return { chunks, remaining };
      };
      const startTtsStream = (turnId, timing) => {
        ttsStreamState = { turnId, buffer: '', lastText: '', timing, timingUsed: false };
      };
      const handleTtsDelta = (text, timing, { final=false } = {}) => {
        if (!speechEnabled) return;
        if (!ttsStreamState || ttsStreamState.turnId !== activeTurnId) return;
        const appended = appendDeltaText(ttsStreamState.lastText, text || '');
        if (text !== undefined) ttsStreamState.lastText = text || ttsStreamState.lastText;
        if (appended) ttsStreamState.buffer += appended;
        const { chunks, remaining } = splitTtsChunks(ttsStreamState.buffer, final);
        ttsStreamState.buffer = remaining;
        chunks.forEach((chunk) => {
          const trimmed = chunk.trim();
          if (!trimmed) return;
          const useTiming = (!ttsStreamState.timingUsed && timing) ? timing : null;
          if (useTiming) ttsStreamState.timingUsed = true;
          queueReply(trimmed, useTiming);
        });
        if (final) ttsStreamState = null;
        if (chunks.length) tryPlayTts();
      };
      const addMessage = (role, text) => {
        const div = document.createElement('div');
        div.className = `msg ${role}`;
        div.textContent = `${role === 'user' ? 'You' : 'Clawd'}: ${text}`;
        chat.appendChild(div);
        chat.scrollTop = chat.scrollHeight;
        return div;
      };
      const updateAssistant = (text) => {
        if (!currentAssistantEl) currentAssistantEl = addMessage('assistant', '');
        currentAssistantEl.textContent = `Clawd: ${text}`;
        chat.scrollTop = chat.scrollHeight;
      };

      const loadConfig = () => {
        const saved = localStorage.getItem('voice-chat-config');
        if (saved) {
          try {
            const parsed = JSON.parse(saved);
            Object.assign(config, parsed);
            if (parsed?.voice === 'nova') config.voice = 'cedar';
          } catch {}
        }
        if (!config.instructions) config.instructions = DEFAULT_TTS_INSTRUCTIONS;
        if (!config.voice) config.voice = 'cedar';
      };
      const saveConfig = () => localStorage.setItem('voice-chat-config', JSON.stringify(config));
      const loadSpeechPref = () => {
        const saved = localStorage.getItem('voice-chat-speech');
        if (saved === 'off') speechEnabled = false;
        if (speechToggle) speechToggle.textContent = speechEnabled ? 'Speech: On' : 'Speech: Off';
      };
      const saveSpeechPref = () => localStorage.setItem('voice-chat-speech', speechEnabled ? 'on' : 'off');
      const updateEndpointUi = () => {
        if (endToggle) endToggle.textContent = autoEndpointing ? 'End: Auto' : 'End: Manual';
        if (endTurnBtn) endTurnBtn.style.display = autoEndpointing ? 'none' : 'inline-block';
        if (endTurnBtn) endTurnBtn.disabled = autoEndpointing || !pendingSpeech || isRecording;
      };
      const loadEndpointPref = () => {
        const saved = localStorage.getItem('voice-chat-endpoint');
        if (saved === 'manual') autoEndpointing = false;
        updateEndpointUi();
      };
      const saveEndpointPref = () => localStorage.setItem('voice-chat-endpoint', autoEndpointing ? 'auto' : 'manual');
      let voicesCache = {};
      const fetchVoices = async (provider) => {
        if (voicesCache[provider]) return voicesCache[provider];
        try {
          const resp = await fetch(`/api/tts/voices?provider=${provider}`);
          if (!resp.ok) return [];
          const data = await resp.json();
          let voices = [];
          if (provider === 'kokoro' && data.voices) {
            // Kokoro returns voices grouped by language: { "English": [{name, id}, ...], ... }
            if (typeof data.voices === 'object' && !Array.isArray(data.voices)) {
              for (const [lang, list] of Object.entries(data.voices)) {
                if (Array.isArray(list)) {
                  for (const v of list) voices.push({ id: v.id || v.name || v, name: v.name || v.id || v, lang });
                }
              }
            } else if (Array.isArray(data.voices)) {
              voices = data.voices.map(v => typeof v === 'string' ? { id: v, name: v } : v);
            }
          } else if (Array.isArray(data.voices)) {
            voices = data.voices.map(v => typeof v === 'string' ? { id: v, name: v } : v);
          }
          voicesCache[provider] = voices;
          return voices;
        } catch (err) {
          console.warn('Failed to fetch voices', err);
          return [];
        }
      };
      const populateVoiceSelect = (voices, selectedVoice) => {
        const select = $('voice');
        select.innerHTML = '';
        if (!voices.length) {
          const opt = document.createElement('option');
          opt.value = selectedVoice || '';
          opt.textContent = selectedVoice || '(none)';
          select.appendChild(opt);
          return;
        }
        let lastLang = null;
        let optgroup = null;
        for (const v of voices) {
          if (v.lang && v.lang !== lastLang) {
            optgroup = document.createElement('optgroup');
            optgroup.label = v.lang;
            select.appendChild(optgroup);
            lastLang = v.lang;
          }
          const opt = document.createElement('option');
          opt.value = v.id;
          opt.textContent = v.name || v.id;
          if (v.id === selectedVoice) opt.selected = true;
          (optgroup || select).appendChild(opt);
        }
        // If selectedVoice not found, select first
        if (selectedVoice && !select.value) select.selectedIndex = 0;
      };
      const updateProviderUi = async (provider) => {
        const instrLabel = $('ttsInstructionsLabel');
        if (instrLabel) instrLabel.style.display = provider === 'kokoro' ? 'none' : '';
        const voices = await fetchVoices(provider);
        const defaultVoice = provider === 'kokoro' ? 'af_heart' : 'cedar';
        const currentVoice = config.ttsProvider === provider ? (config.voice || defaultVoice) : defaultVoice;
        populateVoiceSelect(voices, currentVoice);
      };
      const fillConfigForm = () => {
        $('sessionKey').value = config.sessionKey || '';
        $('ttsProvider').value = config.ttsProvider || 'openai';
        if (ttsInstructions) ttsInstructions.value = config.instructions || '';
        updateProviderUi(config.ttsProvider || 'openai');
      };
      const applyFormToConfig = () => {
        config.sessionKey = $('sessionKey').value.trim() || 'main';
        config.ttsProvider = $('ttsProvider').value || 'openai';
        config.voice = $('voice').value.trim() || (config.ttsProvider === 'kokoro' ? 'af_heart' : 'cedar');
        config.instructions = (ttsInstructions?.value || '').trim();
      };
      const ensureConfig = async (force=false) => {
        loadConfig();
        if (force || !config.sessionKey) {
          fillConfigForm();
          configDialog.showModal();
          return false;
        }
        return true;
      };

      const sendText = async (text) => {
        const msg = text?.trim();
        if (!msg) return;
        const ok = await ensureConfig();
        if (!ok) return;
        const turnId = beginUserTurn('text');
        addMessage('user', msg);
        if (textInput) textInput.value = '';
        try {
          setStatus('thinking');
          const useDeltaTts = STREAM_TTS_ON_DELTA && speechEnabled;
          if (useDeltaTts) startTtsStream(turnId, null);
          const reply = await sendMessage(msg, {
            onDelta: (partial) => {
              if (partial) updateAssistant(partial);
              if (useDeltaTts) handleTtsDelta(partial, null);
            },
            onFinal: (finalText) => {
              if (useDeltaTts) handleTtsDelta(finalText, null, { final: true });
            }
          });
          if (reply) {
            updateAssistant(reply);
            lastReplyText = reply;
            const superseded = turnId !== activeTurnId;
            if (!superseded && speechEnabled) {
              if (audioUnlocked) {
                if (!useDeltaTts) {
                  queueReply(reply, null);
                  await tryPlayTts();
                } else if (ttsStreamState && ttsStreamState.turnId === turnId) {
                  handleTtsDelta(reply, null, { final: true });
                }
              } else {
                setStatus('listening', 'Tap Start to enable audio');
              }
            } else if (!superseded && running) {
              setStatus('listening');
            } else if (!superseded) {
              setStatus('stopped');
            }
          }
          currentAssistantEl = null;
        } catch (error) {
          console.error('Text send error:', error);
          setStatus('error', error?.message || String(error) || 'Text send error');
        }
      };

      const float32ToWav = (samples, sampleRate=SAMPLE_RATE) => {
        const buffer = new ArrayBuffer(44 + samples.length * 2);
        const view = new DataView(buffer);
        const writeStr = (offset, str) => { for (let i=0; i<str.length; i++) view.setUint8(offset+i, str.charCodeAt(i)); };
        writeStr(0,'RIFF'); view.setUint32(4,36+samples.length*2,true); writeStr(8,'WAVE'); writeStr(12,'fmt ');
        view.setUint32(16,16,true); view.setUint16(20,1,true); view.setUint16(22,1,true); view.setUint32(24,sampleRate,true);
        view.setUint32(28,sampleRate*2,true); view.setUint16(32,2,true); view.setUint16(34,16,true); writeStr(36,'data');
        view.setUint32(40,samples.length*2,true);
        for (let i=0; i<samples.length; i++) { const s=Math.max(-1,Math.min(1,samples[i])); view.setInt16(44+i*2, s<0?s*0x8000:s*0x7fff, true); }
        return new Blob([buffer], {type:'audio/wav'});
      };

      const blobToBase64 = async (blob) => {
        const buf = await blob.arrayBuffer();
        const bytes = new Uint8Array(buf);
        let binary = '';
        for (let i=0; i<bytes.length; i++) binary += String.fromCharCode(bytes[i]);
        return btoa(binary);
      };

      const transcribe = async (wavBlob) => {
        const audioBase64 = await blobToBase64(wavBlob);
        const response = await fetch('/api/transcribe', {
          method:'POST',
          headers:{'Content-Type':'application/json'},
          body: JSON.stringify({ audioBase64, mimeType:'audio/wav' })
        });
        if (!response.ok) {
          const text = await response.text().catch(() => '');
          throw new Error(`Transcribe error ${response.status}: ${text || 'no body'}`);
        }
        let data;
        try {
          data = await response.json();
        } catch (err) {
          throw new Error(`Transcribe parse error: ${String(err)}`);
        }
        return data.text || '';
      };

      const sendMessage = async (text, { onDelta, onFinal } = {}) => {
        const response = await fetch('/api/chat/stream', {
          method:'POST',
          headers:{'Content-Type':'application/json'},
          body: JSON.stringify({ text, sessionKey: config.sessionKey, mode: 'voice' })
        });
        if (!response.ok) {
          const body = await response.text().catch(() => '');
          throw new Error(`Chat error ${response.status}: ${body || 'no body'}`);
        }
        if (!response.body) return '';
        const reader = response.body.getReader();
        const decoder = new TextDecoder();
        let buffer = '';
        let eventName = 'message';
        let dataLines = [];
        let lastText = '';
        let finalText = '';
        const flushEvent = () => {
          if (!dataLines.length) return;
          const dataStr = dataLines.join('\n');
          let payload = null;
          try {
            payload = JSON.parse(dataStr);
          } catch {
            payload = { text: dataStr };
          }
          const text = payload?.text ?? '';
          if (eventName === 'delta') {
            if (text) lastText = text;
            if (onDelta) onDelta(text, payload);
          } else if (eventName === 'final') {
            finalText = text || lastText;
            if (onFinal) onFinal(finalText, payload);
          } else if (eventName === 'error') {
            const message = payload?.error || payload?.message || dataStr || 'Chat stream error';
            throw new Error(message);
          }
          dataLines = [];
          eventName = 'message';
        };
        while (true) {
          const { value, done } = await reader.read();
          if (done) break;
          buffer += decoder.decode(value, { stream: true });
          let idx;
          while ((idx = buffer.indexOf('\n')) !== -1) {
            let line = buffer.slice(0, idx);
            buffer = buffer.slice(idx + 1);
            if (line.endsWith('\r')) line = line.slice(0, -1);
            if (!line) {
              flushEvent();
              continue;
            }
            if (line.startsWith('event:')) {
              eventName = line.slice(6).trim();
            } else if (line.startsWith('data:')) {
              dataLines.push(line.slice(5).trimStart());
            }
          }
        }
        flushEvent();
        return finalText || lastText || '';
      };

      const ensureAudioContext = async () => {
        if (!audioCtx) audioCtx = new (window.AudioContext || window.webkitAudioContext)();
        if (audioCtx.state === 'suspended') await audioCtx.resume();
        window.audioCtx = audioCtx;
      };
      const primeAutoplay = () => {
        if (autoplayPrimed) return;
        autoplayPrimed = true;
        try {
          const a = new Audio();
          autoplayPrimeAudio = a;
          a.src = 'data:audio/wav;base64,UklGRiQAAABXQVZFZm10IBAAAAABAAEAESsAACJWAAACABAAZGF0YQAAAAA=';
          // Synchronous play call within user gesture ‚Äî don't await
          const p = a.play();
          if (p) p.then(() => { a.pause(); }).catch(() => {});
        } catch (err) {
          console.warn('Autoplay prime failed', err);
        }
      };

      const unlockAudio = async () => {
        await ensureAudioContext();
        const buffer = audioCtx.createBuffer(1, 1, 22050);
        const source = audioCtx.createBufferSource();
        source.buffer = buffer;
        source.connect(audioCtx.destination);
        source.start(0);
        audioUnlocked = true;
        await primeAutoplay();
      };

      const requestMicPermission = async () => {
        if (!navigator.mediaDevices?.getUserMedia) return false;
        try {
          const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
          stream.getTracks().forEach(track => track.stop());
          return true;
        } catch (err) {
          console.warn('Mic permission failed', err);
          setStatus('error', 'Mic permission blocked');
          return false;
        }
      };

      const padBuffer = (buffer, preMs=80, postMs=120) => {
        const pre = Math.floor(buffer.sampleRate * (preMs/1000));
        const post = Math.floor(buffer.sampleRate * (postMs/1000));
        const padded = audioCtx.createBuffer(buffer.numberOfChannels, buffer.length + pre + post, buffer.sampleRate);
        for (let ch=0; ch<buffer.numberOfChannels; ch++) {
          padded.getChannelData(ch).set(buffer.getChannelData(ch), pre);
        }
        return padded;
      };

      const canStreamTts = () => {
        // Kokoro returns WAV (not streamable via MediaSource); use buffer-based playback
        if ((config.ttsProvider || 'openai') === 'kokoro') return false;
        if (!window.MediaSource || typeof MediaSource.isTypeSupported !== 'function') return false;
        const mime = 'audio/mpeg';
        const audioOk = !!document.createElement('audio').canPlayType(mime);
        if (!audioOk) return false;
        if (!MediaSource.isTypeSupported(mime)) return false;
        const isSafari = /^((?!chrome|android).)*safari/i.test(navigator.userAgent || '');
        if (isSafari) return false;
        return true;
      };

      const streamTtsPlayback = async (text, timing, requestId, controller) => {
        const payload = { text, voice: config.voice, provider: config.ttsProvider || 'openai' };
        if (config.ttsProvider !== 'kokoro' && config.instructions) payload.instructions = config.instructions;
        const response = await fetch('/api/tts/stream', {
          method:'POST',
          headers:{'Content-Type':'application/json'},
          body: JSON.stringify(payload),
          signal: controller.signal
        });
        if (!response.ok || !response.body) {
          const body = await response.text().catch(() => '');
          throw new Error(`TTS stream error ${response.status}: ${body || 'no body'}`);
        }

        const mediaSource = new MediaSource();
        const audio = new Audio();
        currentAudio = audio;
        let objectUrl = '';
        try { if (lastAudioUrl) URL.revokeObjectURL(lastAudioUrl); } catch {}
        objectUrl = URL.createObjectURL(mediaSource);
        lastAudioUrl = objectUrl;
        audio.src = objectUrl;
        // replyAudio element removed

        let sourceBuffer = null;
        let queue = [];
        let streamDone = false;
        let appending = false;
        let firstChunkLogged = false;

        const pump = () => {
          if (!sourceBuffer || appending) return;
          if (queue.length === 0) {
            if (streamDone && !sourceBuffer.updating && mediaSource.readyState === 'open') {
              try { mediaSource.endOfStream(); } catch {}
            }
            return;
          }
          appending = true;
          try {
            sourceBuffer.appendBuffer(queue.shift());
          } catch (err) {
            appending = false;
            throw err;
          }
        };

        mediaSource.addEventListener('sourceopen', () => {
          if (mediaSource.readyState !== 'open') return;
          sourceBuffer = mediaSource.addSourceBuffer('audio/mpeg');
          sourceBuffer.mode = 'sequence';
          sourceBuffer.addEventListener('updateend', () => {
            appending = false;
            pump();
          });
          pump();
        }, { once: true });

        isSpeaking = true;
        const cleanup = (reason) => {
          if (currentAudio === audio) {
            try { audio.pause(); } catch {}
            currentAudio = null;
          }
          isSpeaking = false;
          if (mediaSource?.readyState === 'open') {
            try { mediaSource.endOfStream(); } catch {}
          }
          if (reason) console.log(`[tts] stream cleanup${reason ? ` (${reason})` : ''}`);
        };

        audio.onended = () => {
          cleanup();
          if (running) setStatus('listening');
          tryPlayTts();
        };
        let streamError = null;
        audio.onerror = () => {
          streamError = new Error('Audio element error');
          cleanup('audio-error');
        };

        let playbackStarted = false;
        const startPlayback = async () => {
          if (playbackStarted) return;
          playbackStarted = true;
          try { await audio.play(); }
          catch (err) {
            cleanup('play-error');
            throw err;
          }
        };

        try {
          const reader = response.body.getReader();
          while (true) {
            const { value, done } = await reader.read();
            if (done) break;
            if (currentTtsRequestId !== requestId) {
              try { await reader.cancel(); } catch {}
              cleanup('superseded');
              break;
            }
            if (streamError) throw streamError;
            if (!value || value.length === 0) continue;
            if (!firstChunkLogged) {
              firstChunkLogged = true;
              if (timing) {
                timing.ttsEndAt = Date.now();
                logTurnTiming(timing);
              }
              await startPlayback();
            }
            queue.push(value);
            pump();
          }
          streamDone = true;
          pump();
        } catch (err) {
          cleanup('stream-failed');
          throw err;
        } finally {
          if (currentTtsRequestId === requestId) {
            isTtsFetching = false;
            currentTtsAbort = null;
          }
        }
      };

      const speak = async (text, timing) => {
        if (!text) return;
        setStatus('speaking');
        lastTtsStartAt = Date.now();
        const requestId = ++currentTtsRequestId;
        const controller = new AbortController();
        currentTtsAbort = controller;
        isTtsFetching = true;
        if (timing) timing.ttsStartAt = Date.now();
        try {
          await ensureAudioContext();
          audioUnlocked = audioCtx?.state === 'running';
          if (!audioUnlocked && unlockBtn) unlockBtn.style.display = 'inline-block';

          if (canStreamTts()) {
            try {
              await streamTtsPlayback(text, timing, requestId, controller);
              return;
            } catch (err) {
              console.warn('TTS stream failed, falling back', err);
              if (currentTtsRequestId !== requestId) return;
            }
          }

          const payload = { text, voice: config.voice, provider: config.ttsProvider || 'openai' };
          if (config.ttsProvider !== 'kokoro' && config.instructions) payload.instructions = config.instructions;
          const response = await fetch('/api/tts', {
            method:'POST',
            headers:{'Content-Type':'application/json'},
            body: JSON.stringify(payload),
            signal: controller.signal
          });
          if (!response.ok) {
            const body = await response.text().catch(() => '');
            throw new Error(`TTS error ${response.status}: ${body || 'no body'}`);
          }
          const arrayBuffer = await response.arrayBuffer();
          if (currentTtsRequestId !== requestId) return;
          try { if (lastAudioUrl) URL.revokeObjectURL(lastAudioUrl); } catch {}
          const mimeType = (config.ttsProvider || 'openai') === 'kokoro' ? 'audio/wav' : 'audio/mpeg';
          const blob = new Blob([arrayBuffer], { type: mimeType });
          const url = URL.createObjectURL(blob);
          lastAudioUrl = url;
          // replyAudio element removed
          let buffer = null;
          try {
            buffer = await audioCtx.decodeAudioData(arrayBuffer.slice(0));
          } catch (decodeErr) {
            console.warn('Audio decode failed, falling back to HTMLAudio', decodeErr);
          }
          if (currentTtsRequestId !== requestId) return;
          if (timing) {
            timing.ttsEndAt = Date.now();
            logTurnTiming(timing);
          }
          if (buffer) {
            const padded = padBuffer(buffer);
            if (currentSource) { try { currentSource.stop(); } catch {} }
            const source = audioCtx.createBufferSource();
            currentSource = source;
            isSpeaking = true;
            isTtsFetching = false;
            currentTtsAbort = null;
            source.buffer = padded;
            source.connect(audioCtx.destination);
            source.onended = () => {
              isSpeaking = false;
              currentSource = null;
              if (running) setStatus('listening');
              tryPlayTts();
            };
            source.start(0);
          } else {
            const url = lastAudioUrl;
            const audio = new Audio(url);
            currentAudio = audio;
            isSpeaking = true;
            isTtsFetching = false;
            currentTtsAbort = null;
            audio.onended = () => {
              isSpeaking = false;
              currentAudio = null;
              if (running) setStatus('listening');
              tryPlayTts();
            };
            await audio.play();
          }
        } catch (err) {
          if (err?.name === 'AbortError') {
            stopTtsPlayback('abort');
            if (timing) {
              timing.ttsEndAt = Date.now();
              logTurnTiming(timing, { ttsAborted: true });
            }
            return;
          }
          console.error('TTS playback failed', err);
          isSpeaking = false;
          currentSource = null;
          isTtsFetching = false;
          currentTtsAbort = null;
          if (timing) {
            timing.ttsEndAt = Date.now();
            logTurnTiming(timing, { ttsSkipped: true });
          }
          if (unlockBtn) unlockBtn.style.display = 'inline-block';
          setStatus('error', `Audio error: ${err?.message || String(err)}`);
          tryPlayTts();
        } finally {
          if (currentTtsRequestId === requestId) {
            isTtsFetching = false;
            currentTtsAbort = null;
          }
        }
      };

      const processAudio = async (payload) => {
        const audioSamples = payload?.audio || payload;
        const timing = payload?.timing || { turnId: activeTurnId, vadEndAt: Date.now(), gapMs: lastTurnGapMs };
        try {
          const wavBlob = float32ToWav(audioSamples);
          timing.transcribeStartAt = Date.now();
          const transcript = (await transcribe(wavBlob)).trim();
          timing.transcribeEndAt = Date.now();
          if (!transcript) return;
          updateAdaptiveEndpoint(transcript);
          addMessage('user', transcript);
          setStatus('thinking');
          timing.chatStartAt = Date.now();
          const useDeltaTts = STREAM_TTS_ON_DELTA && speechEnabled;
          if (useDeltaTts) startTtsStream(timing.turnId || activeTurnId, timing);
          const reply = await sendMessage(transcript, {
            onDelta: (partial) => {
              if (partial) updateAssistant(partial);
              if (useDeltaTts) handleTtsDelta(partial, timing);
            },
            onFinal: (finalText) => {
              if (useDeltaTts) handleTtsDelta(finalText, timing, { final: true });
            }
          });
          timing.chatEndAt = Date.now();
          if (reply) {
            updateAssistant(reply);
            lastReplyText = reply;
            const superseded = timing.turnId && timing.turnId !== activeTurnId;
            if (!superseded && speechEnabled) {
              if (audioUnlocked) {
                if (!useDeltaTts) {
                  queueReply(reply, timing);
                  await tryPlayTts();
                } else if (ttsStreamState && ttsStreamState.turnId === (timing.turnId || activeTurnId)) {
                  handleTtsDelta(reply, timing, { final: true });
                }
              } else {
                setStatus('listening', 'Tap Start to enable audio');
                logTurnTiming(timing, { ttsSkipped: true });
              }
            } else if (!superseded && running) {
              setStatus('listening');
              logTurnTiming(timing, { ttsSkipped: true });
            } else if (!superseded) {
              setStatus('stopped');
              logTurnTiming(timing, { ttsSkipped: true });
            } else {
              console.log(`[queue] drop reply for turn ${timing.turnId} (superseded by ${activeTurnId})`);
              logTurnTiming(timing, { ttsSkipped: true });
            }
          }
          currentAssistantEl = null;
        } catch (error) {
          console.error('Processing error:', error);
          setStatus('error', error?.message || String(error) || 'Processing error');
        }
      };

      const processNextAudio = async () => {
        if (busy || audioQueue.length === 0) return;
        busy = true;
        setStatus('processing');
        const next = audioQueue.shift();
        await processAudio(next);
        busy = false;
        if (running && status === 'processing') setStatus('listening');
        if (audioQueue.length > 0) processNextAudio();
        tryPlayTts();
      };

      // Endpointing: delay commit so short pauses can be merged into one turn.
      const finalizePendingSpeech = () => {
        if (!pendingSpeech) return;
        const turnId = pendingSpeech.turnId || activeTurnId;
        if (turnId === lastCommittedTurnId) {
          console.log(`[endpoint] skip duplicate commit for turn ${turnId}`);
          pendingSpeech = null;
          pendingSpeechMeta = null;
          updateEndpointUi();
          return;
        }
        const merged = mergeAudioChunks(pendingSpeech.chunks);
        const timing = {
          turnId,
          gapMs: pendingSpeech.gapMs ?? null,
          speechMs: pendingSpeech.speechMs || 0,
          vadEndAt: Date.now()
        };
        pendingSpeech = null;
        pendingSpeechMeta = null;
        lastCommittedTurnId = turnId;
        lastCommitAt = Date.now();
        lastTurnEndAt = timing.vadEndAt;
        audioQueue.push({ audio: merged, timing });
        processNextAudio();
        updateEndpointUi();
      };
      const scheduleEndpointCommit = () => {
        if (!pendingSpeech) return;
        if (!autoEndpointing) {
          updateEndpointUi();
          return;
        }
        if (endpointTimer) clearTimeout(endpointTimer);
        const silenceMs = computeEndpointMs(pendingSpeech.speechMs || 0);
        endpointTimer = setTimeout(() => {
          endpointTimer = null;
          if (!running) return;
          finalizePendingSpeech();
        }, silenceMs);
      };
      const endTurn = () => {
        if (!pendingSpeech || isRecording) return;
        if (endpointTimer) { clearTimeout(endpointTimer); endpointTimer = null; }
        finalizePendingSpeech();
      };

      const startVAD = async () => {
        if (vadInstance) return;
        vadInstance = await vad.MicVAD.new({
          positiveSpeechThreshold: 0.85,
          minSpeechFrames: 12,
          preSpeechPadFrames: 2,
          onSpeechStart: () => {
            const now = Date.now();
            if (currentAudio) { currentAudio.pause(); currentAudio = null; }
            if (isSpeaking || isTtsFetching) {
              const inHoldoff = lastTtsStartAt && (now - lastTtsStartAt < BARGE_IN_HOLDOFF_MS);
              if (inHoldoff) {
                console.log('[vad] ignore barge-in (holdoff)');
                return;
              }
              bargeInStartAt = now;
              if (bargeInTimer) clearTimeout(bargeInTimer);
              bargeInTimer = setTimeout(() => {
                bargeInTimer = null;
                const elapsed = Date.now() - bargeInStartAt;
                if ((isSpeaking || isTtsFetching) && isRecording && elapsed >= BARGE_IN_MIN_MS) {
                  stopTtsPlayback('barge-in');
                  abortTtsFetch('barge-in');
                  clearPendingReplies('barge-in');
                } else {
                  console.log('[vad] ignore barge-in (short)');
                }
              }, BARGE_IN_CONFIRM_MS);
            }
            if (!pendingSpeech) {
              lastTurnGapMs = lastTurnEndAt ? now - lastTurnEndAt : null;
              if (lastTurnGapMs != null) console.log(`[turn] gap ${Math.round(lastTurnGapMs)}ms`);
              const turnId = beginUserTurn('speech');
              pendingSpeechMeta = { turnId, gapMs: lastTurnGapMs };
            }
            if (endpointTimer) { clearTimeout(endpointTimer); endpointTimer = null; }
            isRecording = true;
            setStatus('recording');
            updateEndpointUi();
          },
          onSpeechEnd: async (audio) => {
            if (!running) return;
            isRecording = false;
            if (bargeInTimer) { clearTimeout(bargeInTimer); bargeInTimer = null; }
            if (!pendingSpeech) {
              pendingSpeech = {
                chunks: [],
                speechMs: 0,
                turnId: pendingSpeechMeta?.turnId || activeTurnId,
                gapMs: pendingSpeechMeta?.gapMs ?? null
              };
              pendingSpeechMeta = null;
            }
            pendingSpeech.chunks.push(audio);
            pendingSpeech.speechMs += (audio.length / SAMPLE_RATE) * 1000;
            scheduleEndpointCommit();
            if (!autoEndpointing && running) {
              setStatus('listening', 'Waiting for end turn');
            }
            updateEndpointUi();
          },
          onnxWASMBasePath:'https://cdn.jsdelivr.net/npm/onnxruntime-web@1.22.0/dist/',
          baseAssetPath:'https://cdn.jsdelivr.net/npm/@ricky0123/vad-web@0.0.29/dist/'
        });
        vadInstance.start();
      };

      const start = async () => {
        if (running || starting) return;
        starting = true;
        startBtn.disabled = true;
        // Prime autoplay IMMEDIATELY in click handler ‚Äî before any awaits
        primeAutoplay();
        setStatus('listening', 'Starting...');
        try {
          const ok = await ensureConfig();
          if (!ok) {
            startBtn.disabled = false;
            setStatus('stopped');
            return;
          }
          const micPromise = requestMicPermission();
          const unlockPromise = unlockAudio().catch((err) => {
            console.error('Audio unlock failed', err);
            if (unlockBtn) unlockBtn.style.display = 'inline-block';
          });
          const micOk = await micPromise;
          if (!micOk) {
            startBtn.disabled = false;
            return;
          }
          await unlockPromise;
          running = true;
          stopBtn.disabled = false;
          await startVAD();
          if (isMuted && vadInstance) vadInstance.pause();
          setStatus(isMuted ? 'stopped' : 'listening', isMuted ? 'Muted' : undefined);
        } finally {
          starting = false;
          if (!running) startBtn.disabled = false;
        }
      };
      const stop = () => {
        running = false;
        starting = false;
        if (vadInstance) {
          try { vadInstance.pause(); } catch {}
          try { vadInstance.destroy?.(); } catch {}
          vadInstance = null;
        }
        if (currentAudio) { currentAudio.pause(); currentAudio = null; }
        if (endpointTimer) { clearTimeout(endpointTimer); endpointTimer = null; }
        if (bargeInTimer) { clearTimeout(bargeInTimer); bargeInTimer = null; }
        pendingSpeech = null;
        pendingSpeechMeta = null;
        stopTtsPlayback('stop');
        abortTtsFetch('stop');
        audioQueue.length = 0;
        clearPendingReplies('stop');
        isRecording = false;
        lastTurnEndAt = 0;
        lastTurnGapMs = null;
        setStatus('stopped'); startBtn.disabled = false; stopBtn.disabled = true;
        updateEndpointUi();
      };

      startBtn.addEventListener('click', start);
      stopBtn.addEventListener('click', stop);
      if (sendBtn && textInput) {
        sendBtn.addEventListener('click', async () => {
          try { await unlockAudio(); } catch {}
          await sendText(textInput.value);
        });
        textInput.addEventListener('keydown', async (e) => {
          if (e.key === 'Enter') {
            e.preventDefault();
            try { await unlockAudio(); } catch {}
            await sendText(textInput.value);
          }
        });
      }
      if (muteBtn) {
        muteBtn.addEventListener('click', async () => {
          isMuted = !isMuted;
          muteBtn.textContent = isMuted ? 'Unmute Mic' : 'Mute Mic';
          if (isMuted) {
            if (vadInstance) {
              try { vadInstance.pause(); } catch {}
              try { vadInstance.destroy?.(); } catch {}
              vadInstance = null;
            }
            setStatus('stopped', 'Muted');
          } else {
            if (running) await startVAD();
            if (running) {
              if (isSpeaking) setStatus('speaking');
              else if (busy) setStatus('processing');
              else if (isRecording) setStatus('recording');
              else setStatus('listening');
            } else {
              setStatus('stopped');
            }
          }
        });
      }
      if (unlockBtn) {
        unlockBtn.addEventListener('click', async () => {
          try {
            await unlockAudio();
            unlockBtn.style.display = 'none';
            setStatus(running ? 'listening' : 'stopped', 'Audio unlocked');
            await tryPlayTts();
          } catch (err) {
            setStatus('error', `Unlock failed: ${err?.message || String(err)}`);
          }
        });
      }
      if (endToggle) {
        endToggle.addEventListener('click', () => {
          autoEndpointing = !autoEndpointing;
          saveEndpointPref();
          if (!autoEndpointing && endpointTimer) {
            clearTimeout(endpointTimer);
            endpointTimer = null;
          }
          if (autoEndpointing && pendingSpeech && !isRecording) scheduleEndpointCommit();
          if (!autoEndpointing && running && pendingSpeech && !isRecording) {
            setStatus('listening', 'Waiting for end turn');
          }
          updateEndpointUi();
        });
      }
      if (endTurnBtn) {
        endTurnBtn.addEventListener('click', () => {
          endTurn();
        });
      }
      if (speechToggle) {
        speechToggle.addEventListener('click', async () => {
          speechEnabled = !speechEnabled;
          speechToggle.textContent = speechEnabled ? 'Speech: On' : 'Speech: Off';
          saveSpeechPref();
          if (speechEnabled) {
            primeAutoplay();
            try { await ensureAudioContext(); audioUnlocked = true; } catch {}
            await tryPlayTts();
          } else {
            stopTtsPlayback('speech-off');
            abortTtsFetch('speech-off');
            clearPendingReplies('speech-off');
            if (running) {
              if (isRecording) setStatus('recording');
              else if (busy) setStatus('processing');
              else setStatus('listening');
            } else {
              setStatus('stopped');
            }
          }
        });
      }
      configBtn.addEventListener('click', () => { loadConfig(); fillConfigForm(); configDialog.showModal(); });
      cancelConfig.addEventListener('click', () => configDialog.close());
      $('ttsProvider').addEventListener('change', (e) => {
        updateProviderUi(e.target.value);
      });
      configForm.addEventListener('submit', (e) => {
        e.preventDefault();
        applyFormToConfig();
        saveConfig();
        configDialog.close();
      });

      loadConfig();
      loadSpeechPref();
      loadEndpointPref();
      setStatus('stopped');
    })();
  </script>
</body>
</html>
